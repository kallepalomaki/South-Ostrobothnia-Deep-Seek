{
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.12",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "gpu",
      "dataSources": [],
      "dockerImageVersionId": 30919,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": true
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
<<<<<<< HEAD
        "34b9f91e9b7d40b5a31f647024a8d8a4": {
=======
        "8927e13b17a54352a84f62e23681ce1c": {
>>>>>>> 1e7dd18e3a6bd07f6a646df9a95feae4621933ae
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
<<<<<<< HEAD
              "IPY_MODEL_b5103e44e4424c8ba8359c95974b26b9",
              "IPY_MODEL_04cb830d7047474099da3d4375bf24c9",
              "IPY_MODEL_8133c323c02d4d05b3482760ac162cf5"
            ],
            "layout": "IPY_MODEL_6a49f9b283e44dd99300ffb94a0d87c2"
          }
        },
        "b5103e44e4424c8ba8359c95974b26b9": {
=======
              "IPY_MODEL_21197ff61a2a41608ec22e1c5a652a58",
              "IPY_MODEL_aa0d03affee44739a7254fa4942656bb",
              "IPY_MODEL_31c32434249749139d029f184576f0c7"
            ],
            "layout": "IPY_MODEL_2f1bee6cb65c4d8482a626f225074501"
          }
        },
        "21197ff61a2a41608ec22e1c5a652a58": {
>>>>>>> 1e7dd18e3a6bd07f6a646df9a95feae4621933ae
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
<<<<<<< HEAD
            "layout": "IPY_MODEL_8acfd814f1cb4d27bccd505c78777f44",
            "placeholder": "​",
            "style": "IPY_MODEL_fb1e2c9ddc7e4c189d23ac224311f5ee",
            "value": "model.safetensors: 100%"
          }
        },
        "04cb830d7047474099da3d4375bf24c9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5992f10a97f1432ea6220161806e1715",
            "max": 5964186418,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_3273645825964fc3aaabf3c90ea44fb1",
            "value": 5964186418
          }
        },
        "8133c323c02d4d05b3482760ac162cf5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e655047313c946c791b6256d391ea87e",
            "placeholder": "​",
            "style": "IPY_MODEL_b533228064cc4d2e934a9f5da3cc29c8",
            "value": " 5.96G/5.96G [00:38&lt;00:00, 249MB/s]"
          }
        },
        "6a49f9b283e44dd99300ffb94a0d87c2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8acfd814f1cb4d27bccd505c78777f44": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fb1e2c9ddc7e4c189d23ac224311f5ee": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5992f10a97f1432ea6220161806e1715": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3273645825964fc3aaabf3c90ea44fb1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "e655047313c946c791b6256d391ea87e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b533228064cc4d2e934a9f5da3cc29c8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "92ef8ee58bf2458da07996ca6780ed7c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_87a55cfcef5c4766b28f12d5b224387c",
              "IPY_MODEL_8fcaf55cab6c44159b72655b7bfe6159",
              "IPY_MODEL_5ec56d59c56242ce8ee5b939a746ac93"
            ],
            "layout": "IPY_MODEL_db11c88edaf74752bac9656c46cfd6fb"
          }
        },
        "87a55cfcef5c4766b28f12d5b224387c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e5ff91c775934700adf78bd54efda1ec",
            "placeholder": "​",
            "style": "IPY_MODEL_b77095c09bfe4b928c7f8550a4abcd2e",
            "value": "generation_config.json: 100%"
          }
        },
        "8fcaf55cab6c44159b72655b7bfe6159": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2d899aae05644f869ac05f2265d181ae",
            "max": 236,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_ef3a891b13a7456ebbab4b0a100e6863",
            "value": 236
          }
        },
        "5ec56d59c56242ce8ee5b939a746ac93": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7ef507d4170040aaafd843062dfa3de0",
            "placeholder": "​",
            "style": "IPY_MODEL_1b0d2c419f2840319e6b8651716a43f9",
            "value": " 236/236 [00:00&lt;00:00, 15.8kB/s]"
          }
        },
        "db11c88edaf74752bac9656c46cfd6fb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e5ff91c775934700adf78bd54efda1ec": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b77095c09bfe4b928c7f8550a4abcd2e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2d899aae05644f869ac05f2265d181ae": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ef3a891b13a7456ebbab4b0a100e6863": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "7ef507d4170040aaafd843062dfa3de0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1b0d2c419f2840319e6b8651716a43f9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ba5a8adfc13c43d1ba75a460634f54fe": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_d6dd2db7747c43698db47abaaac5192c",
              "IPY_MODEL_47555aa9e1d1418a8cb1c3bc5a2efa71",
              "IPY_MODEL_4ea359ef2b204288ab32496f72ebd0ee"
            ],
            "layout": "IPY_MODEL_70430946d2e8485b931eb22f7fea81bd"
          }
        },
        "d6dd2db7747c43698db47abaaac5192c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_133022c36f874862bfb2fa3fbffcafeb",
            "placeholder": "​",
            "style": "IPY_MODEL_9fd626667dea46ae8619aeee08eac509",
            "value": "tokenizer_config.json: 100%"
          }
        },
        "47555aa9e1d1418a8cb1c3bc5a2efa71": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_41f1a1497c7f4188a9b4e2a0c312d48e",
            "max": 52960,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_36c36cfba6c34f99b61affedf9ace24c",
            "value": 52960
          }
        },
        "4ea359ef2b204288ab32496f72ebd0ee": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0ca977c96b3c40ce8e8c341369e2c2e1",
            "placeholder": "​",
            "style": "IPY_MODEL_860e53e9b1aa42d091dc5218433a4f38",
            "value": " 53.0k/53.0k [00:00&lt;00:00, 5.33MB/s]"
          }
        },
        "70430946d2e8485b931eb22f7fea81bd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "133022c36f874862bfb2fa3fbffcafeb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9fd626667dea46ae8619aeee08eac509": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "41f1a1497c7f4188a9b4e2a0c312d48e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "36c36cfba6c34f99b61affedf9ace24c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "0ca977c96b3c40ce8e8c341369e2c2e1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "860e53e9b1aa42d091dc5218433a4f38": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "0767440a38fa47d2b44657a189d84819": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_2549a24510a643dbb18c88326542a092",
              "IPY_MODEL_3dfb198753044afcaaa4242a1c72ae4d",
              "IPY_MODEL_a144f2dcf70341edaa311279ee0a4272"
            ],
            "layout": "IPY_MODEL_a51a58c49ef54668b020dc2d1d6c2dbd"
          }
        },
        "2549a24510a643dbb18c88326542a092": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b84a26b40bc44ef1af553bd562288fdc",
            "placeholder": "​",
            "style": "IPY_MODEL_cef136ececb241bf997e8301fad514e0",
            "value": "tokenizer.json: 100%"
          }
        },
        "3dfb198753044afcaaa4242a1c72ae4d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7e4b4f619c4e4f3db300ef77000ba3c7",
            "max": 17209530,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_88877f2644714232b8e5973184a971d1",
            "value": 17209530
          }
        },
        "a144f2dcf70341edaa311279ee0a4272": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6a00a7c7cda64b84895c5d746a25f045",
            "placeholder": "​",
            "style": "IPY_MODEL_0af2b9c449124ca58e265e7a95963dee",
            "value": " 17.2M/17.2M [00:00&lt;00:00, 41.3MB/s]"
          }
        },
        "a51a58c49ef54668b020dc2d1d6c2dbd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b84a26b40bc44ef1af553bd562288fdc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cef136ececb241bf997e8301fad514e0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7e4b4f619c4e4f3db300ef77000ba3c7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "88877f2644714232b8e5973184a971d1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "6a00a7c7cda64b84895c5d746a25f045": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0af2b9c449124ca58e265e7a95963dee": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1f993a294f2b4efe9a7fcb16083b863c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_b1be7307531a4a95afd15c2d06d9f580",
              "IPY_MODEL_8aff46ceb85f4fdea2e0c74043e5f8a0",
              "IPY_MODEL_4e6c48e09fe3472f9a26145d3b73db61"
            ],
            "layout": "IPY_MODEL_e4118fa6e6534f089fd6b4622a496581"
          }
        },
        "b1be7307531a4a95afd15c2d06d9f580": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e3deaab27c2941fe952f680de56fdeb4",
            "placeholder": "​",
            "style": "IPY_MODEL_1ff7e971a8fc47648f6de119f9c21284",
            "value": "special_tokens_map.json: 100%"
          }
        },
        "8aff46ceb85f4fdea2e0c74043e5f8a0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_95a7f4c8a73244b69f53255719ba39bc",
            "max": 483,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_4186afea28284162843b028c48c6788b",
            "value": 483
          }
        },
        "4e6c48e09fe3472f9a26145d3b73db61": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_803bb3a1603242eeae56dc9cf1b1711c",
            "placeholder": "​",
            "style": "IPY_MODEL_501e006f2f164bc69e91862ae43ff825",
            "value": " 483/483 [00:00&lt;00:00, 46.7kB/s]"
          }
        },
        "e4118fa6e6534f089fd6b4622a496581": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e3deaab27c2941fe952f680de56fdeb4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1ff7e971a8fc47648f6de119f9c21284": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "95a7f4c8a73244b69f53255719ba39bc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4186afea28284162843b028c48c6788b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "803bb3a1603242eeae56dc9cf1b1711c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "501e006f2f164bc69e91862ae43ff825": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c6de51b4820747499bc26c93f6bbd734": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_0fdf72299a624ed78cea8cab22c247e3",
              "IPY_MODEL_17c1118be0d4405e8730ddd16f4d3f52",
              "IPY_MODEL_c6be5e18c7bc4ac48b2844bea708e550"
            ],
            "layout": "IPY_MODEL_1905d9ed53e14ff080e1497b9ebfe497"
          }
        },
        "0fdf72299a624ed78cea8cab22c247e3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1b81456a51e04aea874642ebf4646d88",
            "placeholder": "​",
            "style": "IPY_MODEL_0b2617f73a5a4f7d89b7b3af472b4aa0",
            "value": "Map: 100%"
          }
        },
        "17c1118be0d4405e8730ddd16f4d3f52": {
=======
            "layout": "IPY_MODEL_5e5f1b63655a4ec7b479f4634067cdd3",
            "placeholder": "​",
            "style": "IPY_MODEL_5b877b99c03f44659ec8ad6079d07801",
            "value": "Map: 100%"
          }
        },
        "aa0d03affee44739a7254fa4942656bb": {
>>>>>>> 1e7dd18e3a6bd07f6a646df9a95feae4621933ae
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
<<<<<<< HEAD
            "layout": "IPY_MODEL_36b34c57c44549f891f247fc9e0ed0dc",
            "max": 306,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_c2769fefd9ab487892ce0bd60b598af0",
            "value": 306
          }
        },
        "c6be5e18c7bc4ac48b2844bea708e550": {
=======
            "layout": "IPY_MODEL_b944732af3004ccfaef4a2691cd82495",
            "max": 306,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_036eac72cb4944dda654d0c5eb0993b5",
            "value": 306
          }
        },
        "31c32434249749139d029f184576f0c7": {
>>>>>>> 1e7dd18e3a6bd07f6a646df9a95feae4621933ae
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
<<<<<<< HEAD
            "layout": "IPY_MODEL_e7871feda2174b05bdfb7b3faed87a7c",
            "placeholder": "​",
            "style": "IPY_MODEL_a0fd0971bbe14263a0a5acc6912ee946",
            "value": " 306/306 [00:00&lt;00:00, 5408.95 examples/s]"
          }
        },
        "1905d9ed53e14ff080e1497b9ebfe497": {
=======
            "layout": "IPY_MODEL_ed89eb8130784397903a65fa0b6da84e",
            "placeholder": "​",
            "style": "IPY_MODEL_cd831f04378f4eb1be9ce2b7c3047f86",
            "value": " 306/306 [00:00&lt;00:00, 12620.15 examples/s]"
          }
        },
        "2f1bee6cb65c4d8482a626f225074501": {
>>>>>>> 1e7dd18e3a6bd07f6a646df9a95feae4621933ae
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
<<<<<<< HEAD
        "1b81456a51e04aea874642ebf4646d88": {
=======
        "5e5f1b63655a4ec7b479f4634067cdd3": {
>>>>>>> 1e7dd18e3a6bd07f6a646df9a95feae4621933ae
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
<<<<<<< HEAD
        "0b2617f73a5a4f7d89b7b3af472b4aa0": {
=======
        "5b877b99c03f44659ec8ad6079d07801": {
>>>>>>> 1e7dd18e3a6bd07f6a646df9a95feae4621933ae
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
<<<<<<< HEAD
        "36b34c57c44549f891f247fc9e0ed0dc": {
=======
        "b944732af3004ccfaef4a2691cd82495": {
>>>>>>> 1e7dd18e3a6bd07f6a646df9a95feae4621933ae
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
<<<<<<< HEAD
        "c2769fefd9ab487892ce0bd60b598af0": {
=======
        "036eac72cb4944dda654d0c5eb0993b5": {
>>>>>>> 1e7dd18e3a6bd07f6a646df9a95feae4621933ae
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
<<<<<<< HEAD
        "e7871feda2174b05bdfb7b3faed87a7c": {
=======
        "ed89eb8130784397903a65fa0b6da84e": {
>>>>>>> 1e7dd18e3a6bd07f6a646df9a95feae4621933ae
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
<<<<<<< HEAD
        "a0fd0971bbe14263a0a5acc6912ee946": {
=======
        "cd831f04378f4eb1be9ce2b7c3047f86": {
>>>>>>> 1e7dd18e3a6bd07f6a646df9a95feae4621933ae
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
<<<<<<< HEAD
        "d321eb815f31449ca861a4afc2aa1b4f": {
=======
        "9e6ee0b6599d47d8b97bd05426328951": {
>>>>>>> 1e7dd18e3a6bd07f6a646df9a95feae4621933ae
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
<<<<<<< HEAD
              "IPY_MODEL_fb4b2376947e41b79a4ecd61bd793b89",
              "IPY_MODEL_f9a583b7db4d4e09abcb6b272cfacac4",
              "IPY_MODEL_09e2ec516bc340fca172aab29fa8acfe"
            ],
            "layout": "IPY_MODEL_3398e94183ea4f5a818e5118cf8dd808"
          }
        },
        "fb4b2376947e41b79a4ecd61bd793b89": {
=======
              "IPY_MODEL_fd19162a2ebe4e3f83302e84925a4dd3",
              "IPY_MODEL_524d72b59bb54fa0b9637744205f3a11",
              "IPY_MODEL_ef700c51b4474237a79c9d2caaac6cc3"
            ],
            "layout": "IPY_MODEL_81ca3194b6b9434e913210e8100af975"
          }
        },
        "fd19162a2ebe4e3f83302e84925a4dd3": {
>>>>>>> 1e7dd18e3a6bd07f6a646df9a95feae4621933ae
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
<<<<<<< HEAD
            "layout": "IPY_MODEL_a2aaa1d87e98478da8273b49952ce8c7",
            "placeholder": "​",
            "style": "IPY_MODEL_5b2c78d0d9a34b74bce5ac414a5a9c41",
            "value": "Generating train split: "
          }
        },
        "f9a583b7db4d4e09abcb6b272cfacac4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1a4e3209945843ef8908e39f178198d0",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_77aa1d63d17e4b4c8eff3c4ffab75247",
            "value": 1
          }
        },
        "09e2ec516bc340fca172aab29fa8acfe": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_683e83cdf4ed426dba9abfb102f2b849",
            "placeholder": "​",
            "style": "IPY_MODEL_b7d9bd9440604d10ba824801f06e7700",
            "value": " 1689/0 [00:00&lt;00:00, 14450.46 examples/s]"
          }
        },
        "3398e94183ea4f5a818e5118cf8dd808": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a2aaa1d87e98478da8273b49952ce8c7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5b2c78d0d9a34b74bce5ac414a5a9c41": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1a4e3209945843ef8908e39f178198d0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "77aa1d63d17e4b4c8eff3c4ffab75247": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "683e83cdf4ed426dba9abfb102f2b849": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b7d9bd9440604d10ba824801f06e7700": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2652d05da10748b4bf0109db11a9de3c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_ab6b85e8d30b4546920428483f7b3b4d",
              "IPY_MODEL_58eb072f84bd4a0da9a57b732063a3c1",
              "IPY_MODEL_74c8b64466164765b1d0d95c5985a1a5"
            ],
            "layout": "IPY_MODEL_db2c2cd2680f468ead01d5b95c30abc9"
          }
        },
        "ab6b85e8d30b4546920428483f7b3b4d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3c6be18c532341469579cb9f28a7b765",
            "placeholder": "​",
            "style": "IPY_MODEL_2970d14ab5a2465a805ee7b0e2492277",
            "value": "Map: 100%"
          }
        },
        "58eb072f84bd4a0da9a57b732063a3c1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ef20ad8561a14a0cb35010942da628d9",
            "max": 1688,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_0fd291f1d57b4b7490c154b5f0689957",
            "value": 1688
          }
        },
        "74c8b64466164765b1d0d95c5985a1a5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_15fbd5a9fa884b3f9ca55383a42309a8",
            "placeholder": "​",
            "style": "IPY_MODEL_e6c41110ae2d457abf6ba284692dd5b9",
            "value": " 1688/1688 [00:00&lt;00:00, 35746.13 examples/s]"
          }
        },
        "db2c2cd2680f468ead01d5b95c30abc9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3c6be18c532341469579cb9f28a7b765": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2970d14ab5a2465a805ee7b0e2492277": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ef20ad8561a14a0cb35010942da628d9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0fd291f1d57b4b7490c154b5f0689957": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "15fbd5a9fa884b3f9ca55383a42309a8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e6c41110ae2d457abf6ba284692dd5b9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8852a6ed2910402289a5ed079b23a64f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_7ac929683e464b5ab29d75a793cf7d06",
              "IPY_MODEL_4595cf672c02400e9e17d4af1b0cbb69",
              "IPY_MODEL_3828f813658e4fadb771aef1f7bab7d0"
            ],
            "layout": "IPY_MODEL_f61b51075a874c1bb3f195f20ce0bc33"
          }
        },
        "7ac929683e464b5ab29d75a793cf7d06": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b85b4d75554a4d619d883ff9e955f4ca",
            "placeholder": "​",
            "style": "IPY_MODEL_4f450b9396ec427d8de96560e457aef0",
            "value": "Unsloth: Tokenizing [&quot;text&quot;] (num_proc=2): 100%"
          }
        },
        "4595cf672c02400e9e17d4af1b0cbb69": {
=======
            "layout": "IPY_MODEL_1acfd56ea16c4e68977494991b4f949b",
            "placeholder": "​",
            "style": "IPY_MODEL_bd699d3d83e14b8f8529370807169f05",
            "value": "Unsloth: Tokenizing [&quot;text&quot;] (num_proc=2): 100%"
          }
        },
        "524d72b59bb54fa0b9637744205f3a11": {
>>>>>>> 1e7dd18e3a6bd07f6a646df9a95feae4621933ae
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
<<<<<<< HEAD
            "layout": "IPY_MODEL_edfe2ea1cf2b486a9438211fbfe67676",
            "max": 306,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_30d9d672bdc945cba67fca8d55526b67",
            "value": 306
          }
        },
        "3828f813658e4fadb771aef1f7bab7d0": {
=======
            "layout": "IPY_MODEL_1db372d15294400cb7442d6500ce1d2c",
            "max": 306,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_1f201bf1c46d4e639eba748ab8e0870f",
            "value": 306
          }
        },
        "ef700c51b4474237a79c9d2caaac6cc3": {
>>>>>>> 1e7dd18e3a6bd07f6a646df9a95feae4621933ae
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
<<<<<<< HEAD
            "layout": "IPY_MODEL_fed2d62168a24665a98ecd132883c5e9",
            "placeholder": "​",
            "style": "IPY_MODEL_37d128105d9d4882b28a70238e834cc6",
            "value": " 306/306 [00:02&lt;00:00, 173.96 examples/s]"
          }
        },
        "f61b51075a874c1bb3f195f20ce0bc33": {
=======
            "layout": "IPY_MODEL_8bf0d8e012ef42cf844c8b633964aef6",
            "placeholder": "​",
            "style": "IPY_MODEL_dcf65fa8af5d4673ab201fee64f01068",
            "value": " 306/306 [00:02&lt;00:00, 167.61 examples/s]"
          }
        },
        "81ca3194b6b9434e913210e8100af975": {
>>>>>>> 1e7dd18e3a6bd07f6a646df9a95feae4621933ae
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
<<<<<<< HEAD
        "b85b4d75554a4d619d883ff9e955f4ca": {
=======
        "1acfd56ea16c4e68977494991b4f949b": {
>>>>>>> 1e7dd18e3a6bd07f6a646df9a95feae4621933ae
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
<<<<<<< HEAD
        "4f450b9396ec427d8de96560e457aef0": {
=======
        "bd699d3d83e14b8f8529370807169f05": {
>>>>>>> 1e7dd18e3a6bd07f6a646df9a95feae4621933ae
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
<<<<<<< HEAD
        "edfe2ea1cf2b486a9438211fbfe67676": {
=======
        "1db372d15294400cb7442d6500ce1d2c": {
>>>>>>> 1e7dd18e3a6bd07f6a646df9a95feae4621933ae
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
<<<<<<< HEAD
        "30d9d672bdc945cba67fca8d55526b67": {
=======
        "1f201bf1c46d4e639eba748ab8e0870f": {
>>>>>>> 1e7dd18e3a6bd07f6a646df9a95feae4621933ae
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
<<<<<<< HEAD
        "fed2d62168a24665a98ecd132883c5e9": {
=======
        "8bf0d8e012ef42cf844c8b633964aef6": {
>>>>>>> 1e7dd18e3a6bd07f6a646df9a95feae4621933ae
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
<<<<<<< HEAD
        "37d128105d9d4882b28a70238e834cc6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c59511ba688b4494986fb3f4250f28a3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_1f9dff4093b14db98371344c3ab48e1d",
              "IPY_MODEL_3511707940ee4d7aa4299282f8d721a6",
              "IPY_MODEL_ffb698e74a1f40cd9aaac25a6f3e53af"
            ],
            "layout": "IPY_MODEL_ded10b9b3fa645ffa67091f468ed1575"
          }
        },
        "1f9dff4093b14db98371344c3ab48e1d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fdf63dfa510e4b8a88d49599218bb139",
            "placeholder": "​",
            "style": "IPY_MODEL_b43b46266db84f1597ef4cf7fffc28c6",
            "value": "Unsloth: Tokenizing [&quot;text&quot;] (num_proc=2): 100%"
          }
        },
        "3511707940ee4d7aa4299282f8d721a6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ca01589e4a31438d843fedb54e57bb20",
            "max": 1688,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_8e0f07b6f52543b5ae7777161a7ca46d",
            "value": 1688
          }
        },
        "ffb698e74a1f40cd9aaac25a6f3e53af": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_41266859e78a4a3fb2cf2c73e55d85fd",
            "placeholder": "​",
            "style": "IPY_MODEL_0053e0066ea34b279083873eb19921c6",
            "value": " 1688/1688 [00:02&lt;00:00, 815.32 examples/s]"
          }
        },
        "ded10b9b3fa645ffa67091f468ed1575": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fdf63dfa510e4b8a88d49599218bb139": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b43b46266db84f1597ef4cf7fffc28c6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ca01589e4a31438d843fedb54e57bb20": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8e0f07b6f52543b5ae7777161a7ca46d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "41266859e78a4a3fb2cf2c73e55d85fd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0053e0066ea34b279083873eb19921c6": {
=======
        "dcf65fa8af5d4673ab201fee64f01068": {
>>>>>>> 1e7dd18e3a6bd07f6a646df9a95feae4621933ae
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
<<<<<<< HEAD
=======
        "<a href=\"https://colab.research.google.com/github/kallepalomaki/South-Ostrobothnia-Deep-Seek/blob/main/deep_seek_southob_blog.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
>>>>>>> 1e7dd18e3a6bd07f6a646df9a95feae4621933ae
        "This a google Colab notebook based on Data camp Deep Seek fine tuning tutorial https://www.datacamp.com/tutorial/fine-tuning-deepseek-r1-reasoning-model which I have adapted to for translating standard Finnish to South Osthrobothnian dialect. Fine tuning data is done in two passes firstly a bit over 300 standard Finnish and South Osthrobothnian dialect sentence pairs and second on bit over 2000 standard Finnish South Osthrobothnia word pairs. South Osthrobothnian sentences are taken from Pekka Pohjanpää's blog: http://pohopekka.blogspot.com/\" I'm not sharing the sentence pairs but Standard Finnish that correspond the blog text. Blog text is indexed by sentence order so that standard Finnish sentences can be matched. I also share the vocabulary. Shared data is placeded at directory data.\n",
        "Not that the blog is Copyrighted material and I haven't reached copyright holder. Therfore I provide only a download function. Use at your own responsibility."
      ],
      "metadata": {
        "id": "XmdmEWx70sQp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!pip install unsloth\n",
        "!pip install --force-reinstall --no-cache-dir --no-deps git+https://github.com/unslothai/unsloth.git\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-03-15T07:22:13.068851Z",
          "iopub.execute_input": "2025-03-15T07:22:13.069244Z",
          "iopub.status.idle": "2025-03-15T07:25:24.544624Z",
          "shell.execute_reply.started": "2025-03-15T07:22:13.069203Z",
          "shell.execute_reply": "2025-03-15T07:25:24.543408Z"
        },
        "id": "ZcTI6dtVSlVH"
      },
      "outputs": [],
      "execution_count": 1
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install bs4"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6N7yA6AXtVrw",
<<<<<<< HEAD
        "outputId": "dc358ee0-d027-4e73-add4-5e23a7743e2b"
=======
        "outputId": "19101db0-6ead-4358-a003-2ef9ad5db4eb"
>>>>>>> 1e7dd18e3a6bd07f6a646df9a95feae4621933ae
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
<<<<<<< HEAD
            "Collecting bs4\n",
            "  Downloading bs4-0.0.2-py2.py3-none-any.whl.metadata (411 bytes)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.11/dist-packages (from bs4) (4.13.3)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4->bs4) (2.6)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4->bs4) (4.13.1)\n",
            "Downloading bs4-0.0.2-py2.py3-none-any.whl (1.2 kB)\n",
            "Installing collected packages: bs4\n",
            "Successfully installed bs4-0.0.2\n"
=======
            "Requirement already satisfied: bs4 in /usr/local/lib/python3.11/dist-packages (0.0.2)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.11/dist-packages (from bs4) (4.13.3)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4->bs4) (2.6)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4->bs4) (4.12.2)\n"
>>>>>>> 1e7dd18e3a6bd07f6a646df9a95feae4621933ae
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Directory settings. Adapt the as needed. Secrets are needed for huggingface and wandb tokens. Datadir is needed for storing General Finnish - South Ostrobotnian vocabulary data in file vocabulary_deep_seek_format.json and dialect sentence indices against standard Finnish sentences file in standard_finnish_sentences.json. I'm using google colab, and therefore directories point to google drive."
      ],
      "metadata": {
        "id": "nES17CfOzY_o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "SECRETS_DIR = \"/content/drive/My Drive/secrets\"\n",
        "DATA_DIR = \"/content/drive/My Drive/data\""
      ],
      "metadata": {
        "id": "mOr0dyQ-nRFK"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "secret_file_path = SECRETS_DIR+'/huggingfacetoken.key'\n",
        "with open(secret_file_path) as f:\n",
        "  hf_token=f.read().strip()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 242
        },
        "id": "4O7nKpcBU6rm",
<<<<<<< HEAD
        "outputId": "5478095b-9a32-4a03-d925-1f300891730f"
      },
      "execution_count": 17,
=======
        "outputId": "f5595065-8524-4ce8-e29b-d3eecbbec64d"
      },
      "execution_count": 4,
>>>>>>> 1e7dd18e3a6bd07f6a646df9a95feae4621933ae
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'SECRETS_DIR' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-17-7992eef99a0d>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mdrive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0msecret_file_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSECRETS_DIR\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'/huggingfacetoken.key'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msecret_file_path\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m   \u001b[0mhf_token\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'SECRETS_DIR' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "secret_file_path = SECRETS_DIR+'/wandb.key'\n",
        "\n",
        "with open(secret_file_path) as f:\n",
        "  wb_token=f.read().strip()"
      ],
      "metadata": {
        "id": "yDscdP60WmxU"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import login\n",
        "login(hf_token)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-03-15T07:25:24.546101Z",
          "iopub.execute_input": "2025-03-15T07:25:24.546462Z",
          "iopub.status.idle": "2025-03-15T07:25:25.639298Z",
          "shell.execute_reply.started": "2025-03-15T07:25:24.546426Z",
          "shell.execute_reply": "2025-03-15T07:25:25.638659Z"
        },
        "id": "8c9U8C9oSlVJ"
      },
      "outputs": [],
      "execution_count": 6
    },
    {
      "cell_type": "markdown",
      "source": [
        "Wandb is used here to monitor training. You'll need to create account and get token to use it."
      ],
      "metadata": {
        "id": "uzBjvP0T1S0k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import wandb\n",
        "import os\n",
        "os.environ[\"WANDB_SILENT\"] = \"true\"\n",
        "\n",
        "wandb.login(key=wb_token)\n",
        "run = wandb.init(\n",
        "    project='Fine-tune-DeepSeek-R1-Distill-Llama-8B on South Ostrobotnia dialect 2',\n",
        "    job_type=\"training\",\n",
        "    anonymous=\"allow\"\n",
        ")"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-03-15T07:25:25.640609Z",
          "iopub.execute_input": "2025-03-15T07:25:25.640838Z",
          "iopub.status.idle": "2025-03-15T07:25:40.059852Z",
          "shell.execute_reply.started": "2025-03-15T07:25:25.640818Z",
          "shell.execute_reply": "2025-03-15T07:25:40.059023Z"
        },
        "id": "RHuN_n8HSlVK"
      },
      "outputs": [],
      "execution_count": 7
    },
    {
      "cell_type": "markdown",
      "source": [
        "Deep Seek model is downloaded from Huggingface. You'll need Huggingface api key for this."
      ],
      "metadata": {
        "id": "LEKl3h3Sy3ke"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from unsloth import FastLanguageModel\n",
        "\n",
        "max_seq_length = 2048\n",
        "dtype = None\n",
        "load_in_4bit = True\n",
        "\n",
        "\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name = \"unsloth/DeepSeek-R1-Distill-Llama-8B\",\n",
        "    max_seq_length = max_seq_length,\n",
        "    dtype = dtype,\n",
        "    load_in_4bit = load_in_4bit,\n",
        "    token = hf_token,\n",
        ")"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-03-15T07:25:40.061016Z",
          "iopub.execute_input": "2025-03-15T07:25:40.061335Z",
          "iopub.status.idle": "2025-03-15T07:26:48.153053Z",
          "shell.execute_reply.started": "2025-03-15T07:25:40.061312Z",
          "shell.execute_reply": "2025-03-15T07:26:48.152432Z"
        },
        "colab": {
<<<<<<< HEAD
          "base_uri": "https://localhost:8080/",
          "height": 326,
          "referenced_widgets": [
            "34b9f91e9b7d40b5a31f647024a8d8a4",
            "b5103e44e4424c8ba8359c95974b26b9",
            "04cb830d7047474099da3d4375bf24c9",
            "8133c323c02d4d05b3482760ac162cf5",
            "6a49f9b283e44dd99300ffb94a0d87c2",
            "8acfd814f1cb4d27bccd505c78777f44",
            "fb1e2c9ddc7e4c189d23ac224311f5ee",
            "5992f10a97f1432ea6220161806e1715",
            "3273645825964fc3aaabf3c90ea44fb1",
            "e655047313c946c791b6256d391ea87e",
            "b533228064cc4d2e934a9f5da3cc29c8",
            "92ef8ee58bf2458da07996ca6780ed7c",
            "87a55cfcef5c4766b28f12d5b224387c",
            "8fcaf55cab6c44159b72655b7bfe6159",
            "5ec56d59c56242ce8ee5b939a746ac93",
            "db11c88edaf74752bac9656c46cfd6fb",
            "e5ff91c775934700adf78bd54efda1ec",
            "b77095c09bfe4b928c7f8550a4abcd2e",
            "2d899aae05644f869ac05f2265d181ae",
            "ef3a891b13a7456ebbab4b0a100e6863",
            "7ef507d4170040aaafd843062dfa3de0",
            "1b0d2c419f2840319e6b8651716a43f9",
            "ba5a8adfc13c43d1ba75a460634f54fe",
            "d6dd2db7747c43698db47abaaac5192c",
            "47555aa9e1d1418a8cb1c3bc5a2efa71",
            "4ea359ef2b204288ab32496f72ebd0ee",
            "70430946d2e8485b931eb22f7fea81bd",
            "133022c36f874862bfb2fa3fbffcafeb",
            "9fd626667dea46ae8619aeee08eac509",
            "41f1a1497c7f4188a9b4e2a0c312d48e",
            "36c36cfba6c34f99b61affedf9ace24c",
            "0ca977c96b3c40ce8e8c341369e2c2e1",
            "860e53e9b1aa42d091dc5218433a4f38",
            "0767440a38fa47d2b44657a189d84819",
            "2549a24510a643dbb18c88326542a092",
            "3dfb198753044afcaaa4242a1c72ae4d",
            "a144f2dcf70341edaa311279ee0a4272",
            "a51a58c49ef54668b020dc2d1d6c2dbd",
            "b84a26b40bc44ef1af553bd562288fdc",
            "cef136ececb241bf997e8301fad514e0",
            "7e4b4f619c4e4f3db300ef77000ba3c7",
            "88877f2644714232b8e5973184a971d1",
            "6a00a7c7cda64b84895c5d746a25f045",
            "0af2b9c449124ca58e265e7a95963dee",
            "1f993a294f2b4efe9a7fcb16083b863c",
            "b1be7307531a4a95afd15c2d06d9f580",
            "8aff46ceb85f4fdea2e0c74043e5f8a0",
            "4e6c48e09fe3472f9a26145d3b73db61",
            "e4118fa6e6534f089fd6b4622a496581",
            "e3deaab27c2941fe952f680de56fdeb4",
            "1ff7e971a8fc47648f6de119f9c21284",
            "95a7f4c8a73244b69f53255719ba39bc",
            "4186afea28284162843b028c48c6788b",
            "803bb3a1603242eeae56dc9cf1b1711c",
            "501e006f2f164bc69e91862ae43ff825"
          ]
        },
        "id": "LfcJ0NfGSlVL",
        "outputId": "530ad267-8fe1-4790-e6e0-9a7d8a0ebeb9"
=======
          "base_uri": "https://localhost:8080/"
        },
        "id": "LfcJ0NfGSlVL",
        "outputId": "57f6e2df-a039-4e42-a6e7-3a4304ed1b01"
>>>>>>> 1e7dd18e3a6bd07f6a646df9a95feae4621933ae
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🦥 Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
            "🦥 Unsloth Zoo will now patch everything to make training faster!\n",
<<<<<<< HEAD
            "==((====))==  Unsloth 2025.3.19: Fast Llama patching. Transformers: 4.50.3.\n",
=======
            "==((====))==  Unsloth 2025.3.17: Fast Llama patching. Transformers: 4.48.3.\n",
>>>>>>> 1e7dd18e3a6bd07f6a646df9a95feae4621933ae
            "   \\\\   /|    Tesla T4. Num GPUs = 1. Max memory: 14.741 GB. Platform: Linux.\n",
            "O^O/ \\_/ \\    Torch: 2.6.0+cu124. CUDA: 7.5. CUDA Toolkit: 12.4. Triton: 3.2.0\n",
            "\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.29.post3. FA2 = False]\n",
            " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
            "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
          ]
<<<<<<< HEAD
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/5.96G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "34b9f91e9b7d40b5a31f647024a8d8a4"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "generation_config.json:   0%|          | 0.00/236 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "92ef8ee58bf2458da07996ca6780ed7c"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/53.0k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ba5a8adfc13c43d1ba75a460634f54fe"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/17.2M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "0767440a38fa47d2b44657a189d84819"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/483 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "1f993a294f2b4efe9a7fcb16083b863c"
            }
          },
          "metadata": {}
=======
>>>>>>> 1e7dd18e3a6bd07f6a646df9a95feae4621933ae
        }
      ],
      "execution_count": 8
    },
    {
      "cell_type": "markdown",
      "source": [
        "Prompt for testing with standard Finnish how that translates to the dialect."
      ],
      "metadata": {
        "id": "KfXlhJc8zMs6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt_style = \"\"\"Below is an instruction that describes a task, paired with an input that provides further context.\n",
        "Write a response that appropriately completes the request.\n",
        "\n",
        "### Instruction:\n",
        "Translate standard Finnish sentence to South Ostrobothnian dialect.\n",
        "\n",
        "### Question:\n",
        "{}\n",
        "\n",
        "### Response:\n",
        "<think>{}\"\"\""
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-03-15T07:26:48.153880Z",
          "iopub.execute_input": "2025-03-15T07:26:48.154083Z",
          "iopub.status.idle": "2025-03-15T07:26:48.158203Z",
          "shell.execute_reply.started": "2025-03-15T07:26:48.154065Z",
          "shell.execute_reply": "2025-03-15T07:26:48.157412Z"
        },
        "id": "RbrLUXQISlVL"
      },
      "outputs": [],
      "execution_count": 9
    },
    {
      "cell_type": "markdown",
      "source": [
        "Run test on the original model before finetuning."
      ],
      "metadata": {
        "id": "2j6mcjGBzacW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "standard_finnish = \"Hevoskauppias ajoi uudella hevosellaan meidän talon ohi. Katsoin häntä aitan ovelta.\"\n",
        "\n",
        "FastLanguageModel.for_inference(model)\n",
        "inputs = tokenizer([prompt_style.format(standard_finnish, \"\")], return_tensors=\"pt\").to(\"cuda\")\n",
        "\n",
        "outputs = model.generate(\n",
        "    input_ids=inputs.input_ids,\n",
        "    attention_mask=inputs.attention_mask,\n",
        "    max_new_tokens=1200,\n",
        "    use_cache=True,\n",
        ")\n",
        "response = tokenizer.batch_decode(outputs)\n",
        "print(\"Dialect:\")\n",
        "print(response[0].split(\"### Response:\")[1])"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-03-15T07:26:48.159087Z",
          "iopub.execute_input": "2025-03-15T07:26:48.159374Z",
          "iopub.status.idle": "2025-03-15T07:27:14.274869Z",
          "shell.execute_reply.started": "2025-03-15T07:26:48.159352Z",
          "shell.execute_reply": "2025-03-15T07:27:14.274072Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_BddXNWJSlVN",
<<<<<<< HEAD
        "outputId": "9296400c-d317-449f-8f9b-062026c0d611"
=======
        "outputId": "cf5491e1-9792-4d56-af3a-9f5afc907c65"
>>>>>>> 1e7dd18e3a6bd07f6a646df9a95feae4621933ae
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dialect:\n",
            "\n",
            "<think>\n",
<<<<<<< HEAD
            "Okay, so I need to translate the given Finnish sentence into the South Ostrobothnian dialect. Let me start by understanding the original sentence and then figure out how it would be phrased in the dialect.\n",
            "\n",
            "The original sentence is: \"Hevoskauppias ajoi uudella hevosellaan meidän talon ohi. Katsoin häntä aitan ovelta.\"\n",
            "\n",
            "Breaking it down, \"Hevoskauppias\" means a horse trader or horse dealer. \"Ajoi\" is the verb meaning \"drove\" or \"went.\" \"Uudella hevosellaan\" is \"with a new horse.\" \"Meidän talon ohi\" translates to \"around our farm\" or \"near our house.\" \n",
            "\n",
            "The second sentence is: \"Katsoin häntä aitan ovelta.\" \"Katsoin\" is \"looked\" or \"saw.\" \"Häntä\" is \"her,\" \"aita\" is \"the gate,\" and \"ovelta\" is \"from the window.\" So, \"saw her from the window.\"\n",
            "\n",
            "Now, translating this into the South Ostrobothnian dialect. I think in this dialect, some words might be more localized or have different forms. For example, \"hevoskauppias\" might be called \"hevoskauppi\" in the dialect, dropping the last syllable. \"Ajoi\" could be \"ajoitui\" or \"ajoitti,\" but I'm not sure. Maybe it's just \"ajoitti.\" \n",
            "\n",
            "\"Uudella hevosellaan\" would likely remain similar, maybe \"uudella hevosella.\" \"Meidän talon ohi\" might become \"meidän talon lähelle\" or \"meidän talon äärelle,\" but \"talon ohi\" is already quite colloquial. \n",
            "\n",
            "For the second sentence, \"Katsoin häntä aitan ovelta\" would become \"Katsoin häntä aitaan ovenlta.\" The word \"ovelta\" in Finnish is \"from the window,\" so in the dialect, it might stay as \"ovenlta.\"\n",
            "\n",
            "Putting it all together, the translation might be: \"Hevoskauppi ajoi uudella hevosella meidän talon lähelle. Katsoin häntä aitaan ovenlta.\"\n",
            "\n",
            "I think that's a reasonable attempt. Maybe I missed some dialect-specific terms, but this should be close.\n",
            "</think>\n",
            "\n",
            "Hevoskauppi ajoi uudella hevosella meidän talon lähelle. Katsoin häntä aitaan ovenlta.<｜end▁of▁sentence｜>\n"
=======
            "Okay, so I have this task where I need to translate a standard Finnish sentence into the South Ostrobothnian dialect. Let me break this down step by step. First, I need to understand the original sentence and then figure out how it would be expressed in the dialect.\n",
            "\n",
            "The original sentence is: \"Hevoskauppias ajoi uudella hevosellaan meidän talon ohi. Katsoin häntä aitan ovelta.\"\n",
            "\n",
            "Alright, let's parse this sentence. The first part is \"Hevoskauppias ajoi uudella hevosellaan meidän talon ohi.\" This seems to be about a horse dealer who is driving a new horse past their house. The second part is \"Katsoin häntä aitan ovelta,\" which translates to \"I watched him from the window.\"\n",
            "\n",
            "Now, translating this into South Ostrobothnian dialect. I know that dialects often have unique words and structures compared to standard language. So, I need to identify the key components and see how they might be expressed differently.\n",
            "\n",
            "Starting with \"Hevoskauppias.\" In the dialect, I think \"kauppias\" might be a term of endearment or a nickname, so perhaps it would be \"Hevosen kauppias\" or something similar, but I'm not sure. Maybe it's just \"Hevoskauppias\" as is.\n",
            "\n",
            "Next, \"ajoi uudella hevosellaan.\" \"Ajoi\" means \"drove\" or \"drove away.\" \"Uudella\" is \"new,\" and \"hevosellaan\" is \"with the new horse.\" So, in dialect, maybe \"ajoi uudella hevosellaan\" could be \"ajoi uudell' hevosellaan,\" with an apostrophe for the \"j\" sound.\n",
            "\n",
            "Then, \"meidän talon ohi.\" \"Meidän\" is \"our,\" \"talon\" is \"house,\" and \"ohi\" is \"by\" or \"near.\" In dialect, \"talon ohi\" might be \"talon ohi,\" but perhaps \"meidän talon ohi\" could be simplified or have a different structure.\n",
            "\n",
            "The second sentence, \"Katsoin häntä aitan ovelta,\" translates to \"I watched him from the window.\" In dialect, \"aivan ovelta\" might become \"aivan ovelta,\" but perhaps \"aivan\" is used differently. Maybe \"aivan ovelta\" is the same, but \"häntä\" is just \"him.\"\n",
            "\n",
            "Putting it all together, the translated sentence might be:\n",
            "\n",
            "\"Hevoskauppias ajoi uudell' hevosellaan meidän talon ohi. Katsoin häntä aivan ovelta.\"\n",
            "\n",
            "I think that's a reasonable attempt. I might have missed some nuances, but this seems to capture the essence of the original sentence in the dialect.\n",
            "</think>\n",
            "\n",
            "Hevoskauppias ajoi uudell' hevosellaan meidän talon ohi. Katsoin häntä aivan ovelta.\n",
            "\n",
            "---\n",
            "\n",
            "### Translation:\n",
            "The horse dealer drove the new horse past our house. I watched him from the window.<｜end▁of▁sentence｜>\n"
>>>>>>> 1e7dd18e3a6bd07f6a646df9a95feae4621933ae
          ]
        }
      ],
      "execution_count": 10
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training prompt for standard Finnish and dialect sentence pairs."
      ],
      "metadata": {
        "id": "etmh2N30zumu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_prompt_style = \"\"\"Below is an instruction that describes a task, paired with an input that provides further context.\n",
        "Write a response that appropriately completes the request.\n",
        "\n",
        "### Instruction:\n",
        "Translate standard Finnish sentence to South Ostrobothnian dialect.\n",
        "\n",
        "### Question:\n",
        "{}\n",
        "\n",
        "### Response:\n",
        "<think>\n",
        "{}\n",
        "</think>\n",
        "{}\"\"\""
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-03-15T07:27:14.275729Z",
          "iopub.execute_input": "2025-03-15T07:27:14.276003Z",
          "iopub.status.idle": "2025-03-15T07:27:14.280362Z",
          "shell.execute_reply.started": "2025-03-15T07:27:14.275980Z",
          "shell.execute_reply": "2025-03-15T07:27:14.279462Z"
        },
        "id": "zfwfPSYTSlVN"
      },
      "outputs": [],
<<<<<<< HEAD
      "execution_count": 22
=======
      "execution_count": 11
>>>>>>> 1e7dd18e3a6bd07f6a646df9a95feae4621933ae
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training prompt for standard Finnish to dialect vocabulary."
      ],
      "metadata": {
        "id": "LKCCZtvszz4y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_prompt_vocabulary_style = \"\"\"Below is an instruction that describes a task, paired with an input that provides further context.\n",
        "Write a response that appropriately completes the request.\n",
        "\n",
        "### Instruction:\n",
        "Translate the following word from standard Finnish to South Ostrobothnian dialect.\n",
        "\n",
        "### Word:\n",
        "{}\n",
        "\n",
        "### Dialect Equivalent:\n",
        "<think>\n",
        "{}\n",
        "</think>\n",
        "{}\"\"\""
      ],
      "metadata": {
        "id": "ve2t9bAc9jHf"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Formatting function for standard Finnish to dialect sentence pairs."
      ],
      "metadata": {
        "id": "33RVEZyu0B9d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "EOS_TOKEN = tokenizer.eos_token  # Must add EOS_TOKEN\n",
        "\n",
        "\n",
        "def formatting_prompts_func(examples):\n",
        "    inputs = examples[\"Question\"]\n",
        "    cots = examples[\"Complex_CoT\"]\n",
        "    outputs = examples[\"Response\"]\n",
        "    texts = []\n",
        "    for input, cot, output in zip(inputs, cots, outputs):\n",
        "        text = train_prompt_style.format(input, cot, output) + EOS_TOKEN\n",
        "        texts.append(text)\n",
        "    return {\n",
        "        \"text\": texts,\n",
        "    }"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-03-15T07:27:14.282413Z",
          "iopub.execute_input": "2025-03-15T07:27:14.282686Z",
          "iopub.status.idle": "2025-03-15T07:27:14.297201Z",
          "shell.execute_reply.started": "2025-03-15T07:27:14.282658Z",
          "shell.execute_reply": "2025-03-15T07:27:14.296527Z"
        },
        "id": "uwjS6ZQHSlVO"
      },
      "outputs": [],
      "execution_count": 13
    },
    {
      "cell_type": "markdown",
      "source": [
        "Formatting function for vocabulary prompts."
      ],
      "metadata": {
        "id": "MUbQ-5dY0KZd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def formatting_prompts_vocab_func(examples):\n",
        "    inputs = examples[\"Question\"]\n",
        "    cots = examples[\"Complex_CoT\"]\n",
        "    outputs = examples[\"Response\"]\n",
        "    texts = []\n",
        "    for input, cot, output in zip(inputs, cots, outputs):\n",
        "        text = train_prompt_vocabulary_style.format(input, cot, output) + EOS_TOKEN\n",
        "        texts.append(text)\n",
        "    return {\n",
        "        \"text\": texts,\n",
        "    }"
      ],
      "metadata": {
        "id": "qQmt7wJHMbK1"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Function to get Pekka Pohjanpää's blog. The blog is split into sentence level and arranged as list of dictionaries."
      ],
      "metadata": {
        "id": "00B4K5cP0O7Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import json\n",
        "def get_pohopekka():\n",
        "    main_page_link=\"http://pohopekka.blogspot.com/\"\n",
        "    structured_data = []\n",
        "    page=requests.get(main_page_link)\n",
        "    soup = BeautifulSoup(page.content, 'html.parser')\n",
        "    for element in soup.find_all([\"h2\", \"div\"]):\n",
        "        if element.name==\"h2\":\n",
        "            latest_h2 = element.text.strip()\n",
        "        elif element.name==\"div\" and \"blogPost\" in element.get(\"class\", []):\n",
        "            if latest_h2:\n",
        "                content=element.text.split(\"# posted\")[0].strip().replace('\\r',' ').replace('PEKAN SIVUPERSOONA','').\\\n",
        "                    replace('Pekan Sivupersoona','').replace(\"\\n\",\" \")\n",
        "\n",
        "                content=content.split(\"----\",1)[0].strip()\n",
        "\n",
        "                if len(content)>5:\n",
        "                    structured_data.append({\"title\": latest_h2, \"content\": content})\n",
        "                latest_h2 = None\n",
        "\n",
        "    return structured_data"
      ],
      "metadata": {
        "id": "NVx-SiWNmlX8"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Matching Pekka Pohjanpää's story on dialect to standard Finnish based on precalculated index."
      ],
      "metadata": {
        "id": "6kV_BPIi0k0Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def match_story_sentences_to_standard(stories,standard_finnish_sentences):\n",
        "    story_idx=0\n",
        "    dialect=dict()\n",
        "    for story_ in stories:\n",
        "        story=story_[\"content\"]\n",
        "        story_splitted=story.split(\". \")\n",
        "        inside_story_idx=0\n",
        "        for sentence in story_splitted:\n",
        "            dialect[str(story_idx) + \"-\" + str(inside_story_idx)] = sentence\n",
        "            inside_story_idx+=1\n",
        "        story_idx+=1\n",
        "\n",
        "\n",
        "    with open(standard_finnish_sentences) as f:\n",
        "        standard=json.load(f)\n",
        "\n",
        "    standard_dialect=[]\n",
        "\n",
        "    for key in standard:\n",
        "        standard_dialect.append({\"standard\":standard[key],\"dialect\":dialect[key]})\n",
        "    return standard_dialect"
      ],
      "metadata": {
        "id": "Qyac9aa7mnox"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Function to compose standard dialect prompts."
      ],
      "metadata": {
        "id": "z7l0raBc1XAr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def write_translated(standard_dialect):\n",
        "    data_arr=[]\n",
        "    for data in standard_dialect:\n",
        "        target_data=data[\"dialect\"].strip()\n",
        "        input_data=data[\"standard\"].strip()\n",
        "        system_dict=dict()\n",
        "        input_dict=dict()\n",
        "        target_dict=dict()\n",
        "        system_dict[\"role\"] = \"system\"\n",
        "        system_dict[\"content\"]=\"You translate standard Finnish sentences into the South Ostrobothnian dialect.\"\n",
        "        input_dict[\"role\"] = \"user\"\n",
        "        input_dict[\"content\"] = \"Ilmaise seuraava Etelä-Pohjanmaan murteella: \" + input_data\n",
        "\n",
        "        target_dict[\"role\"] = \"assistant\"\n",
        "        target_dict[\"content\"] = target_data\n",
        "        data_arr.append({\n",
        "            \"Question\": input_data,  # Standard Finnish\n",
        "            \"Complex_CoT\": \"\",\n",
        "            \"Response\": target_data,  # South Ostrobothnian dialect\n",
        "            \"text\": input_data + \" - \" + target_data  # Same as Question and Response\n",
        "        })\n",
        "\n",
        "\n",
        "    return data_arr"
      ],
      "metadata": {
        "id": "zMrYwxwsm20y"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stories=get_pohopekka()\n",
        "standard_dialect=match_story_sentences_to_standard(stories=stories,standard_finnish_sentences=DATA_DIR+\"/standard_finnish_sentences.json\")\n",
        "train_set=write_translated(standard_dialect=standard_dialect)\n",
        "random.shuffle(train_set)"
      ],
      "metadata": {
        "id": "xasgcCLqm8OJ"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Run standard Finnish vs. dialect data generation."
      ],
      "metadata": {
        "id": "Pv5oRm3r1pLf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset, Dataset\n",
        "\n",
        "dataset0 = Dataset.from_list(train_set)\n",
        "\n",
        "dataset0 = dataset0.map(formatting_prompts_func, batched = True,)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-03-15T07:27:14.298412Z",
          "iopub.execute_input": "2025-03-15T07:27:14.298697Z",
          "iopub.status.idle": "2025-03-15T07:27:19.550058Z",
          "shell.execute_reply.started": "2025-03-15T07:27:14.298667Z",
          "shell.execute_reply": "2025-03-15T07:27:19.549158Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
<<<<<<< HEAD
            "c6de51b4820747499bc26c93f6bbd734",
            "0fdf72299a624ed78cea8cab22c247e3",
            "17c1118be0d4405e8730ddd16f4d3f52",
            "c6be5e18c7bc4ac48b2844bea708e550",
            "1905d9ed53e14ff080e1497b9ebfe497",
            "1b81456a51e04aea874642ebf4646d88",
            "0b2617f73a5a4f7d89b7b3af472b4aa0",
            "36b34c57c44549f891f247fc9e0ed0dc",
            "c2769fefd9ab487892ce0bd60b598af0",
            "e7871feda2174b05bdfb7b3faed87a7c",
            "a0fd0971bbe14263a0a5acc6912ee946"
          ]
        },
        "id": "2RrXytIuSlVO",
        "outputId": "b2b2118d-362c-42c8-9bd7-fb21248457c5"
=======
            "8927e13b17a54352a84f62e23681ce1c",
            "21197ff61a2a41608ec22e1c5a652a58",
            "aa0d03affee44739a7254fa4942656bb",
            "31c32434249749139d029f184576f0c7",
            "2f1bee6cb65c4d8482a626f225074501",
            "5e5f1b63655a4ec7b479f4634067cdd3",
            "5b877b99c03f44659ec8ad6079d07801",
            "b944732af3004ccfaef4a2691cd82495",
            "036eac72cb4944dda654d0c5eb0993b5",
            "ed89eb8130784397903a65fa0b6da84e",
            "cd831f04378f4eb1be9ce2b7c3047f86"
          ]
        },
        "id": "2RrXytIuSlVO",
        "outputId": "294ad53f-5323-4096-c73c-6e7793911201"
>>>>>>> 1e7dd18e3a6bd07f6a646df9a95feae4621933ae
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/306 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
<<<<<<< HEAD
              "model_id": "c6de51b4820747499bc26c93f6bbd734"
=======
              "model_id": "8927e13b17a54352a84f62e23681ce1c"
>>>>>>> 1e7dd18e3a6bd07f6a646df9a95feae4621933ae
            }
          },
          "metadata": {}
        }
      ],
      "execution_count": 19
    },
    {
      "cell_type": "markdown",
      "source": [
        "Get the vocabulary data, which I'm sharing in the repo."
      ],
      "metadata": {
        "id": "HJmw_UCA1tVE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data_file_path = DATA_DIR+'/vocabulary_deep_seek_format.json'\n",
        "\n",
        "\n",
        "dataset1 = load_dataset('json',data_files=data_file_path, split = \"train[:-1]\",trust_remote_code=True)\n",
        "\n",
        "dataset1 = dataset1.map(formatting_prompts_vocab_func, batched = True,)"
      ],
      "metadata": {
<<<<<<< HEAD
        "id": "QuiMNAZo_Ph7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 81,
          "referenced_widgets": [
            "d321eb815f31449ca861a4afc2aa1b4f",
            "fb4b2376947e41b79a4ecd61bd793b89",
            "f9a583b7db4d4e09abcb6b272cfacac4",
            "09e2ec516bc340fca172aab29fa8acfe",
            "3398e94183ea4f5a818e5118cf8dd808",
            "a2aaa1d87e98478da8273b49952ce8c7",
            "5b2c78d0d9a34b74bce5ac414a5a9c41",
            "1a4e3209945843ef8908e39f178198d0",
            "77aa1d63d17e4b4c8eff3c4ffab75247",
            "683e83cdf4ed426dba9abfb102f2b849",
            "b7d9bd9440604d10ba824801f06e7700",
            "2652d05da10748b4bf0109db11a9de3c",
            "ab6b85e8d30b4546920428483f7b3b4d",
            "58eb072f84bd4a0da9a57b732063a3c1",
            "74c8b64466164765b1d0d95c5985a1a5",
            "db2c2cd2680f468ead01d5b95c30abc9",
            "3c6be18c532341469579cb9f28a7b765",
            "2970d14ab5a2465a805ee7b0e2492277",
            "ef20ad8561a14a0cb35010942da628d9",
            "0fd291f1d57b4b7490c154b5f0689957",
            "15fbd5a9fa884b3f9ca55383a42309a8",
            "e6c41110ae2d457abf6ba284692dd5b9"
          ]
        },
        "outputId": "fd5eba39-8452-4cab-a8c0-82f716cf8879"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Generating train split: 0 examples [00:00, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d321eb815f31449ca861a4afc2aa1b4f"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/1688 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "2652d05da10748b4bf0109db11a9de3c"
            }
          },
          "metadata": {}
        }
      ]
=======
        "id": "QuiMNAZo_Ph7"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Model configurations."
      ],
      "metadata": {
        "id": "AfnwuNDZ15hW"
      }
>>>>>>> 1e7dd18e3a6bd07f6a646df9a95feae4621933ae
    },
    {
      "cell_type": "markdown",
      "source": [
        "Model configurations."
      ],
      "metadata": {
        "id": "AfnwuNDZ15hW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = FastLanguageModel.get_peft_model(\n",
        "    model,\n",
        "    r=16,\n",
        "    target_modules=[\n",
        "        \"q_proj\",\n",
        "        \"k_proj\",\n",
        "        \"v_proj\",\n",
        "        \"o_proj\",\n",
        "        \"gate_proj\",\n",
        "        \"up_proj\",\n",
        "        \"down_proj\",\n",
        "    ],\n",
        "    lora_alpha=16,\n",
        "    lora_dropout=0,\n",
        "    bias=\"none\",\n",
        "    use_gradient_checkpointing=\"unsloth\",  # True or \"unsloth\" for very long context\n",
        "    random_state=3407,\n",
        "    use_rslora=False,\n",
        "    loftq_config=None,\n",
        ")"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-03-15T07:27:19.551116Z",
          "iopub.execute_input": "2025-03-15T07:27:19.551444Z",
          "iopub.status.idle": "2025-03-15T07:27:25.644607Z",
          "shell.execute_reply.started": "2025-03-15T07:27:19.551411Z",
          "shell.execute_reply": "2025-03-15T07:27:25.643691Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B3QlpPb4SlVP",
<<<<<<< HEAD
        "outputId": "acedbb4e-1f37-49d9-cb0a-1ab1478df77b"
=======
        "outputId": "18af28fc-cd59-44a7-d58a-52a610dba49b"
>>>>>>> 1e7dd18e3a6bd07f6a646df9a95feae4621933ae
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
<<<<<<< HEAD
            "Unsloth 2025.3.19 patched 32 layers with 32 QKV layers, 32 O layers and 32 MLP layers.\n"
=======
            "Unsloth 2025.3.17 patched 32 layers with 32 QKV layers, 32 O layers and 32 MLP layers.\n"
>>>>>>> 1e7dd18e3a6bd07f6a646df9a95feae4621933ae
          ]
        }
      ],
      "execution_count": 21
    },
    {
      "cell_type": "markdown",
      "source": [
        "Trainer for standard Finnish - dialect sentence pairs."
      ],
      "metadata": {
        "id": "N-6oyi-91-wN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from trl import SFTTrainer\n",
        "from transformers import TrainingArguments\n",
        "from unsloth import is_bfloat16_supported\n",
        "\n",
        "trainer_transl = SFTTrainer(\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    train_dataset=dataset0,\n",
        "    dataset_text_field=\"text\",\n",
        "    max_seq_length=max_seq_length,\n",
        "    dataset_num_proc=2,\n",
        "    args=TrainingArguments(\n",
        "        per_device_train_batch_size=2,\n",
        "        gradient_accumulation_steps=4,\n",
        "        # Use num_train_epochs = 1, warmup_ratio for full training runs!\n",
        "        warmup_steps=5,\n",
        "        #max_steps=60,\n",
        "        num_train_epochs=3,\n",
        "        learning_rate=2e-4,\n",
        "        fp16=not is_bfloat16_supported(),\n",
        "        bf16=is_bfloat16_supported(),\n",
        "        logging_steps=10,\n",
        "        optim=\"adamw_8bit\",\n",
        "        weight_decay=0.01,\n",
        "        lr_scheduler_type=\"linear\",\n",
        "        seed=3407,\n",
        "        output_dir=\"outputs\",\n",
        "    ),\n",
        ")"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-03-15T07:27:25.645541Z",
          "iopub.execute_input": "2025-03-15T07:27:25.645792Z",
          "iopub.status.idle": "2025-03-15T07:27:28.297562Z",
          "shell.execute_reply.started": "2025-03-15T07:27:25.645770Z",
          "shell.execute_reply": "2025-03-15T07:27:28.296498Z"
        },
        "id": "iZ63LanwSlVQ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
<<<<<<< HEAD
            "8852a6ed2910402289a5ed079b23a64f",
            "7ac929683e464b5ab29d75a793cf7d06",
            "4595cf672c02400e9e17d4af1b0cbb69",
            "3828f813658e4fadb771aef1f7bab7d0",
            "f61b51075a874c1bb3f195f20ce0bc33",
            "b85b4d75554a4d619d883ff9e955f4ca",
            "4f450b9396ec427d8de96560e457aef0",
            "edfe2ea1cf2b486a9438211fbfe67676",
            "30d9d672bdc945cba67fca8d55526b67",
            "fed2d62168a24665a98ecd132883c5e9",
            "37d128105d9d4882b28a70238e834cc6"
          ]
        },
        "outputId": "e6e2826b-2813-4681-8ff8-b2f1c23c67c4"
=======
            "9e6ee0b6599d47d8b97bd05426328951",
            "fd19162a2ebe4e3f83302e84925a4dd3",
            "524d72b59bb54fa0b9637744205f3a11",
            "ef700c51b4474237a79c9d2caaac6cc3",
            "81ca3194b6b9434e913210e8100af975",
            "1acfd56ea16c4e68977494991b4f949b",
            "bd699d3d83e14b8f8529370807169f05",
            "1db372d15294400cb7442d6500ce1d2c",
            "1f201bf1c46d4e639eba748ab8e0870f",
            "8bf0d8e012ef42cf844c8b633964aef6",
            "dcf65fa8af5d4673ab201fee64f01068"
          ]
        },
        "outputId": "de43b916-3833-469e-f284-b1e566ccbbf4"
>>>>>>> 1e7dd18e3a6bd07f6a646df9a95feae4621933ae
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Unsloth: Tokenizing [\"text\"] (num_proc=2):   0%|          | 0/306 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
<<<<<<< HEAD
              "model_id": "8852a6ed2910402289a5ed079b23a64f"
=======
              "model_id": "9e6ee0b6599d47d8b97bd05426328951"
>>>>>>> 1e7dd18e3a6bd07f6a646df9a95feae4621933ae
            }
          },
          "metadata": {}
        }
      ],
      "execution_count": 22
    },
    {
      "cell_type": "markdown",
      "source": [
        "Trainer for vocabulary data. Less epochs and smaller learning rate as number of training samples is larger than number of samples on the sentence pair data."
      ],
      "metadata": {
        "id": "wP6vCXvO2EDT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#works when not ran\n",
        "trainer_vocab = SFTTrainer(\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    train_dataset=dataset1,\n",
        "    dataset_text_field=\"text\",\n",
        "    max_seq_length=max_seq_length,\n",
        "    dataset_num_proc=2,\n",
        "    args=TrainingArguments(\n",
        "        per_device_train_batch_size=2,\n",
        "        gradient_accumulation_steps=4,\n",
        "        # Use num_train_epochs = 1, warmup_ratio for full training runs!\n",
        "        warmup_steps=5,\n",
        "        #max_steps=60,\n",
        "        num_train_epochs=1,\n",
        "        learning_rate=5e-5,\n",
        "        fp16=not is_bfloat16_supported(),\n",
        "        bf16=is_bfloat16_supported(),\n",
        "        logging_steps=10,\n",
        "        optim=\"adamw_8bit\",\n",
        "        weight_decay=0.01,\n",
        "        lr_scheduler_type=\"linear\",\n",
        "        seed=3407,\n",
        "        output_dir=\"outputs/vocab\",\n",
        "    ),\n",
        ")"
      ],
      "metadata": {
<<<<<<< HEAD
        "id": "7iTXNTcxTGeM",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "c59511ba688b4494986fb3f4250f28a3",
            "1f9dff4093b14db98371344c3ab48e1d",
            "3511707940ee4d7aa4299282f8d721a6",
            "ffb698e74a1f40cd9aaac25a6f3e53af",
            "ded10b9b3fa645ffa67091f468ed1575",
            "fdf63dfa510e4b8a88d49599218bb139",
            "b43b46266db84f1597ef4cf7fffc28c6",
            "ca01589e4a31438d843fedb54e57bb20",
            "8e0f07b6f52543b5ae7777161a7ca46d",
            "41266859e78a4a3fb2cf2c73e55d85fd",
            "0053e0066ea34b279083873eb19921c6"
          ]
        },
        "outputId": "b4e23091-fb38-467b-a101-71fcc51c9a7e"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Unsloth: Tokenizing [\"text\"] (num_proc=2):   0%|          | 0/1688 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c59511ba688b4494986fb3f4250f28a3"
            }
          },
          "metadata": {}
        }
      ]
=======
        "id": "7iTXNTcxTGeM"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Run trainers."
      ],
      "metadata": {
        "id": "cDGVGX6j2Vsz"
      }
>>>>>>> 1e7dd18e3a6bd07f6a646df9a95feae4621933ae
    },
    {
      "cell_type": "markdown",
      "source": [
        "Run trainers."
      ],
      "metadata": {
        "id": "cDGVGX6j2Vsz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi\n",
        "trainer_stats = trainer_transl.train()"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-03-15T07:27:28.298472Z",
          "iopub.execute_input": "2025-03-15T07:27:28.298742Z",
          "iopub.status.idle": "2025-03-15T08:12:04.935861Z",
          "shell.execute_reply.started": "2025-03-15T07:27:28.298715Z",
          "shell.execute_reply": "2025-03-15T08:12:04.935157Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 886
        },
        "id": "b1OIazATSlVQ",
<<<<<<< HEAD
        "outputId": "fb32e559-28a0-4f26-8012-3f7b4650b85f"
=======
        "outputId": "9fdc07f0-0d6b-4591-825a-1896a10d8543"
>>>>>>> 1e7dd18e3a6bd07f6a646df9a95feae4621933ae
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
<<<<<<< HEAD
            "Sun Apr 13 16:28:05 2025       \n",
=======
            "Wed Mar 19 20:06:32 2025       \n",
>>>>>>> 1e7dd18e3a6bd07f6a646df9a95feae4621933ae
            "+-----------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
            "|-----------------------------------------+------------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                        |               MIG M. |\n",
            "|=========================================+========================+======================|\n",
            "|   0  Tesla T4                       Off |   00000000:00:04.0 Off |                    0 |\n",
<<<<<<< HEAD
            "| N/A   55C    P0             26W /   70W |    6242MiB /  15360MiB |      0%      Default |\n",
=======
            "| N/A   71C    P0             32W /   70W |    6242MiB /  15360MiB |      4%      Default |\n",
>>>>>>> 1e7dd18e3a6bd07f6a646df9a95feae4621933ae
            "|                                         |                        |                  N/A |\n",
            "+-----------------------------------------+------------------------+----------------------+\n",
            "                                                                                         \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                              |\n",
            "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
            "|        ID   ID                                                               Usage      |\n",
            "|=========================================================================================|\n",
            "+-----------------------------------------------------------------------------------------+\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n",
            "   \\\\   /|    Num examples = 306 | Num Epochs = 3 | Total steps = 114\n",
            "O^O/ \\_/ \\    Batch size per device = 2 | Gradient accumulation steps = 4\n",
            "\\        /    Data Parallel GPUs = 1 | Total batch size (2 x 4 x 1) = 8\n",
            " \"-____-\"     Trainable parameters = 41,943,040/8,000,000,000 (0.52% trained)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Unsloth: Will smartly offload gradients to save VRAM!\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='114' max='114' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
<<<<<<< HEAD
              "      [114/114 09:15, Epoch 2/3]\n",
=======
              "      [114/114 10:01, Epoch 2/3]\n",
>>>>>>> 1e7dd18e3a6bd07f6a646df9a95feae4621933ae
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
<<<<<<< HEAD
              "      <td>3.389800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>20</td>\n",
              "      <td>1.764100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>30</td>\n",
              "      <td>1.576100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>40</td>\n",
              "      <td>1.399100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>50</td>\n",
              "      <td>1.174700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>60</td>\n",
              "      <td>1.055500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>70</td>\n",
              "      <td>1.091600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>80</td>\n",
              "      <td>0.942200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>90</td>\n",
              "      <td>0.783300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>0.735900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>110</td>\n",
              "      <td>0.732800</td>\n",
=======
              "      <td>3.245500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>20</td>\n",
              "      <td>1.858500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>30</td>\n",
              "      <td>1.591100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>40</td>\n",
              "      <td>1.441700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>50</td>\n",
              "      <td>1.133700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>60</td>\n",
              "      <td>1.120400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>70</td>\n",
              "      <td>1.041800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>80</td>\n",
              "      <td>0.972400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>90</td>\n",
              "      <td>0.729300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>0.780500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>110</td>\n",
              "      <td>0.711700</td>\n",
>>>>>>> 1e7dd18e3a6bd07f6a646df9a95feae4621933ae
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        }
      ],
      "execution_count": 24
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi\n",
        "trainer_stats = trainer_vocab.train()"
      ],
      "metadata": {
        "id": "2itzLVVWUSgW",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
<<<<<<< HEAD
        "outputId": "6ea1b318-2b73-421e-89a8-bade7808577c"
=======
        "outputId": "3a7e879e-b938-43a7-bdb2-43e20c94833f"
>>>>>>> 1e7dd18e3a6bd07f6a646df9a95feae4621933ae
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
<<<<<<< HEAD
            "Sun Apr 13 16:37:30 2025       \n",
=======
            "Wed Mar 19 20:16:42 2025       \n",
>>>>>>> 1e7dd18e3a6bd07f6a646df9a95feae4621933ae
            "+-----------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
            "|-----------------------------------------+------------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                        |               MIG M. |\n",
            "|=========================================+========================+======================|\n",
            "|   0  Tesla T4                       Off |   00000000:00:04.0 Off |                    0 |\n",
<<<<<<< HEAD
            "| N/A   75C    P0             40W /   70W |    6930MiB /  15360MiB |      3%      Default |\n",
=======
            "| N/A   74C    P0             33W /   70W |    7012MiB /  15360MiB |      3%      Default |\n",
>>>>>>> 1e7dd18e3a6bd07f6a646df9a95feae4621933ae
            "|                                         |                        |                  N/A |\n",
            "+-----------------------------------------+------------------------+----------------------+\n",
            "                                                                                         \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                              |\n",
            "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
            "|        ID   ID                                                               Usage      |\n",
            "|=========================================================================================|\n",
            "+-----------------------------------------------------------------------------------------+\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n",
            "   \\\\   /|    Num examples = 1,688 | Num Epochs = 1 | Total steps = 211\n",
            "O^O/ \\_/ \\    Batch size per device = 2 | Gradient accumulation steps = 4\n",
            "\\        /    Data Parallel GPUs = 1 | Total batch size (2 x 4 x 1) = 8\n",
            " \"-____-\"     Trainable parameters = 41,943,040/8,000,000,000 (0.52% trained)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='211' max='211' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
<<<<<<< HEAD
              "      [211/211 13:39, Epoch 1/1]\n",
=======
              "      [211/211 14:18, Epoch 1/1]\n",
>>>>>>> 1e7dd18e3a6bd07f6a646df9a95feae4621933ae
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
<<<<<<< HEAD
              "      <td>1.243300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>20</td>\n",
              "      <td>0.363500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>30</td>\n",
              "      <td>0.322300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>40</td>\n",
              "      <td>0.329100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>50</td>\n",
              "      <td>0.325300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>60</td>\n",
              "      <td>0.341400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>70</td>\n",
              "      <td>0.327800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>80</td>\n",
              "      <td>0.340000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>90</td>\n",
              "      <td>0.319400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>0.295000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>110</td>\n",
              "      <td>0.305200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>120</td>\n",
              "      <td>0.297300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>130</td>\n",
              "      <td>0.302700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>140</td>\n",
              "      <td>0.291500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>150</td>\n",
              "      <td>0.319700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>160</td>\n",
              "      <td>0.308200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>170</td>\n",
              "      <td>0.325500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>180</td>\n",
              "      <td>0.308000</td>\n",
=======
              "      <td>1.236000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>20</td>\n",
              "      <td>0.363900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>30</td>\n",
              "      <td>0.321300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>40</td>\n",
              "      <td>0.325500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>50</td>\n",
              "      <td>0.322900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>60</td>\n",
              "      <td>0.340200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>70</td>\n",
              "      <td>0.329000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>80</td>\n",
              "      <td>0.336000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>90</td>\n",
              "      <td>0.321800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>0.297000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>110</td>\n",
              "      <td>0.308600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>120</td>\n",
              "      <td>0.298700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>130</td>\n",
              "      <td>0.302900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>140</td>\n",
              "      <td>0.288300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>150</td>\n",
              "      <td>0.319500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>160</td>\n",
              "      <td>0.306900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>170</td>\n",
              "      <td>0.324000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>180</td>\n",
              "      <td>0.306500</td>\n",
>>>>>>> 1e7dd18e3a6bd07f6a646df9a95feae4621933ae
              "    </tr>\n",
              "    <tr>\n",
              "      <td>190</td>\n",
              "      <td>0.281500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>200</td>\n",
<<<<<<< HEAD
              "      <td>0.282800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>210</td>\n",
              "      <td>0.302000</td>\n",
=======
              "      <td>0.287000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>210</td>\n",
              "      <td>0.299700</td>\n",
>>>>>>> 1e7dd18e3a6bd07f6a646df9a95feae4621933ae
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
<<<<<<< HEAD
        "Run test on single sentence."
=======
        "Run test on signle sentence."
>>>>>>> 1e7dd18e3a6bd07f6a646df9a95feae4621933ae
      ],
      "metadata": {
        "id": "l0BRv1et2YaQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "standard_finnish = \"Hevoskauppias ajoi uudella hevosellaan meidän talon ohi. Katsoin häntä aitan ovelta.\"\n",
        "\n",
        "\n",
        "FastLanguageModel.for_inference(model)  # Unsloth has 2x faster inference!\n",
        "inputs = tokenizer([prompt_style.format(standard_finnish, \"\")], return_tensors=\"pt\").to(\"cuda\")\n",
        "\n",
        "outputs = model.generate(\n",
        "    input_ids=inputs.input_ids,\n",
        "    attention_mask=inputs.attention_mask,\n",
        "    max_new_tokens=1200,\n",
        "    use_cache=True,\n",
        ")\n",
        "response = tokenizer.batch_decode(outputs)\n",
        "print(\"Dialect:\")\n",
        "print(response[0].split(\"### Response:\")[1])\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-03-15T08:12:04.936884Z",
          "iopub.execute_input": "2025-03-15T08:12:04.937104Z",
          "iopub.status.idle": "2025-03-15T08:12:32.743352Z",
          "shell.execute_reply.started": "2025-03-15T08:12:04.937085Z",
          "shell.execute_reply": "2025-03-15T08:12:32.742274Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NQtRFraISlVR",
<<<<<<< HEAD
        "outputId": "a0b9aca8-5891-4280-cae8-cf14949c8321"
=======
        "outputId": "0618ff47-1805-4bdb-dc67-f42048fa89fc"
>>>>>>> 1e7dd18e3a6bd07f6a646df9a95feae4621933ae
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dialect:\n",
            "\n",
            "<think>\n",
            "\n",
            "</think>\n",
<<<<<<< HEAD
            "Hevooskauppias ajoo uureella hevoosellaan meirän taloohon. Kattelin sitä aita ovelta<｜end▁of▁sentence｜>\n"
          ]
        }
      ],
      "execution_count": 26
    },
    {
      "cell_type": "markdown",
      "source": [
        "Save and produce a quantized smaller sized model < 10 Gb"
      ],
      "metadata": {
        "id": "jFWJI298OV5p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil\n",
        "new_model_local = \"DeepSeek-R1-South-Ostrobothnia\"\n",
        "model.save_pretrained(new_model_local)\n",
        "tokenizer.save_pretrained(new_model_local)\n",
        "\n",
        "model.save_pretrained_merged(new_model_local, tokenizer, save_method = \"merged_16bit\",)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V6MPw5ue86mA",
        "outputId": "ac74a38a-433a-4fbe-bede-5ed872ad3632"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Unsloth: You have 1 CPUs. Using `safe_serialization` is 10x slower.\n",
            "We shall switch to Pytorch saving, which might take 3 minutes and not 30 minutes.\n",
            "To force `safe_serialization`, set it to `None` instead.\n",
            "Unsloth: Kaggle/Colab has limited disk space. We need to delete the downloaded\n",
            "model which will save 4-16GB of disk space, allowing you to save on Kaggle/Colab.\n",
            "Unsloth: Will remove a cached repo with size 6.0G\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Unsloth: Merging 4bit and LoRA weights to 16bit...\n",
            "Unsloth: Will use up to 5.43 out of 12.67 RAM for saving.\n",
            "Unsloth: Saving model... This might take 5 minutes ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 34%|███▍      | 11/32 [00:00<00:01, 13.90it/s]\n",
            "We will save to Disk and not RAM now.\n",
            "100%|██████████| 32/32 [01:17<00:00,  2.43s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Unsloth: Saving tokenizer... Done.\n",
            "Unsloth: Saving DeepSeek-R1-South-Ostrobothnia/pytorch_model-00001-of-00004.bin...\n",
            "Unsloth: Saving DeepSeek-R1-South-Ostrobothnia/pytorch_model-00002-of-00004.bin...\n",
            "Unsloth: Saving DeepSeek-R1-South-Ostrobothnia/pytorch_model-00003-of-00004.bin...\n",
            "Unsloth: Saving DeepSeek-R1-South-Ostrobothnia/pytorch_model-00004-of-00004.bin...\n",
            "Done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls -l ./DeepSeek-R1-South-Ostrobothnia\n",
        "shutil.make_archive(\"deepseek_model\", 'zip', new_model_local)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 297
        },
        "id": "RE3DI1Ky9DJu",
        "outputId": "6a75e09a-5b2d-48b8-c244-1d2ab7293688"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total 15865040\n",
            "-rw-r--r-- 1 root root        824 Apr 13 16:51 adapter_config.json\n",
            "-rw-r--r-- 1 root root  167832240 Apr 13 16:51 adapter_model.safetensors\n",
            "-rw-r--r-- 1 root root        924 Apr 13 16:52 config.json\n",
            "-rw-r--r-- 1 root root        231 Apr 13 16:52 generation_config.json\n",
            "-rw-r--r-- 1 root root 4976718466 Apr 13 16:53 pytorch_model-00001-of-00004.bin\n",
            "-rw-r--r-- 1 root root 4999826886 Apr 13 16:55 pytorch_model-00002-of-00004.bin\n",
            "-rw-r--r-- 1 root root 4915939082 Apr 13 16:56 pytorch_model-00003-of-00004.bin\n",
            "-rw-r--r-- 1 root root 1168140873 Apr 13 16:56 pytorch_model-00004-of-00004.bin\n",
            "-rw-r--r-- 1 root root      23950 Apr 13 16:56 pytorch_model.bin.index.json\n",
            "-rw-r--r-- 1 root root       5127 Apr 13 16:51 README.md\n",
            "-rw-r--r-- 1 root root        483 Apr 13 16:52 special_tokens_map.json\n",
            "-rw-r--r-- 1 root root      52960 Apr 13 16:52 tokenizer_config.json\n",
            "-rw-r--r-- 1 root root   17209530 Apr 13 16:52 tokenizer.json\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/deepseek_model.zip'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/ggerganov/llama.cpp\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CRuEAUks9h2a",
        "outputId": "3a180649-5215-4045-b550-24811f6e8c10"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'llama.cpp'...\n",
            "remote: Enumerating objects: 48542, done.\u001b[K\n",
            "remote: Counting objects: 100% (2/2), done.\u001b[K\n",
            "remote: Compressing objects: 100% (2/2), done.\u001b[K\n",
            "remote: Total 48542 (delta 0), reused 0 (delta 0), pack-reused 48540 (from 2)\u001b[K\n",
            "Receiving objects: 100% (48542/48542), 102.28 MiB | 26.16 MiB/s, done.\n",
            "Resolving deltas: 100% (34993/34993), done.\n",
            "\u001b[31mERROR: Could not open requirements file: [Errno 2] No such file or directory: 'requirements.txt'\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "os.chdir(\"llama.cpp\")\n",
        "!ls\n",
        "!pip install -r ./requirements.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "4OrDpKVDKNrM",
        "outputId": "3d1b94ca-02e7-440e-c6df-bb1fa53b200d"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "AUTHORS\t\t\t       docs\t   mypy.ini\n",
            "build-xcframework.sh\t       examples    pocs\n",
            "ci\t\t\t       flake.lock  poetry.lock\n",
            "cmake\t\t\t       flake.nix   prompts\n",
            "CMakeLists.txt\t\t       ggml\t   pyproject.toml\n",
            "CMakePresets.json\t       gguf-py\t   pyrightconfig.json\n",
            "CODEOWNERS\t\t       grammars    README.md\n",
            "common\t\t\t       include\t   requirements\n",
            "CONTRIBUTING.md\t\t       LICENSE\t   requirements.txt\n",
            "convert_hf_to_gguf.py\t       licenses    scripts\n",
            "convert_hf_to_gguf_update.py   Makefile    SECURITY.md\n",
            "convert_llama_ggml_to_gguf.py  media\t   src\n",
            "convert_lora_to_gguf.py        models\t   tests\n",
            "Looking in indexes: https://pypi.org/simple, https://download.pytorch.org/whl/cpu, https://download.pytorch.org/whl/cpu, https://download.pytorch.org/whl/cpu, https://download.pytorch.org/whl/cpu\n",
            "Collecting numpy~=1.26.4 (from -r ././requirements/requirements-convert_legacy_llama.txt (line 1))\n",
            "  Downloading numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.0/61.0 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: sentencepiece~=0.2.0 in /usr/local/lib/python3.11/dist-packages (from -r ././requirements/requirements-convert_legacy_llama.txt (line 2)) (0.2.0)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.45.1 in /usr/local/lib/python3.11/dist-packages (from -r ././requirements/requirements-convert_legacy_llama.txt (line 3)) (4.50.3)\n",
            "Collecting gguf>=0.1.0 (from -r ././requirements/requirements-convert_legacy_llama.txt (line 4))\n",
            "  Downloading gguf-0.14.0-py3-none-any.whl.metadata (3.7 kB)\n",
            "Collecting protobuf<5.0.0,>=4.21.0 (from -r ././requirements/requirements-convert_legacy_llama.txt (line 5))\n",
            "  Downloading protobuf-4.25.6-cp37-abi3-manylinux2014_x86_64.whl.metadata (541 bytes)\n",
            "Collecting torch~=2.2.1 (from -r ././requirements/requirements-convert_hf_to_gguf.txt (line 3))\n",
            "  Downloading https://download.pytorch.org/whl/cpu/torch-2.2.2%2Bcpu-cp311-cp311-linux_x86_64.whl (186.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m186.8/186.8 MB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting aiohttp~=3.9.3 (from -r ././requirements/requirements-tool_bench.txt (line 1))\n",
            "  Downloading aiohttp-3.9.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.5 kB)\n",
            "Requirement already satisfied: pytest~=8.3.3 in /usr/local/lib/python3.11/dist-packages (from -r ././requirements/requirements-tool_bench.txt (line 2)) (8.3.5)\n",
            "Collecting huggingface_hub~=0.23.2 (from -r ././requirements/requirements-tool_bench.txt (line 3))\n",
            "  Downloading huggingface_hub-0.23.5-py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: matplotlib~=3.10.0 in /usr/local/lib/python3.11/dist-packages (from -r ././requirements/requirements-tool_bench.txt (line 4)) (3.10.0)\n",
            "Collecting openai~=1.55.3 (from -r ././requirements/requirements-tool_bench.txt (line 6))\n",
            "  Downloading openai-1.55.3-py3-none-any.whl.metadata (24 kB)\n",
            "Collecting pandas~=2.2.3 (from -r ././requirements/requirements-tool_bench.txt (line 7))\n",
            "  Downloading pandas-2.2.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (89 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m89.9/89.9 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting prometheus-client~=0.20.0 (from -r ././requirements/requirements-tool_bench.txt (line 8))\n",
            "  Downloading prometheus_client-0.20.0-py3-none-any.whl.metadata (1.8 kB)\n",
            "Requirement already satisfied: requests~=2.32.3 in /usr/local/lib/python3.11/dist-packages (from -r ././requirements/requirements-tool_bench.txt (line 9)) (2.32.3)\n",
            "Collecting wget~=3.2 (from -r ././requirements/requirements-tool_bench.txt (line 10))\n",
            "  Downloading wget-3.2.zip (10 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: typer~=0.15.1 in /usr/local/lib/python3.11/dist-packages (from -r ././requirements/requirements-tool_bench.txt (line 11)) (0.15.2)\n",
            "Requirement already satisfied: seaborn~=0.13.2 in /usr/local/lib/python3.11/dist-packages (from -r ././requirements/requirements-tool_bench.txt (line 12)) (0.13.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.45.1->-r ././requirements/requirements-convert_legacy_llama.txt (line 3)) (3.18.0)\n",
            "INFO: pip is looking at multiple versions of transformers to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting transformers<5.0.0,>=4.45.1 (from -r ././requirements/requirements-convert_legacy_llama.txt (line 3))\n",
            "  Downloading transformers-4.51.2-py3-none-any.whl.metadata (38 kB)\n",
            "  Downloading transformers-4.51.1-py3-none-any.whl.metadata (38 kB)\n",
            "  Downloading transformers-4.51.0-py3-none-any.whl.metadata (38 kB)\n",
            "  Downloading transformers-4.50.2-py3-none-any.whl.metadata (39 kB)\n",
            "  Downloading transformers-4.50.1-py3-none-any.whl.metadata (39 kB)\n",
            "  Downloading transformers-4.50.0-py3-none-any.whl.metadata (39 kB)\n",
            "  Downloading transformers-4.49.0-py3-none-any.whl.metadata (44 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.0/44.0 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hINFO: pip is still looking at multiple versions of transformers to determine which version is compatible with other requirements. This could take a while.\n",
            "  Downloading transformers-4.48.3-py3-none-any.whl.metadata (44 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.4/44.4 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading transformers-4.48.2-py3-none-any.whl.metadata (44 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.4/44.4 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading transformers-4.48.1-py3-none-any.whl.metadata (44 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.4/44.4 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading transformers-4.48.0-py3-none-any.whl.metadata (44 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.4/44.4 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading transformers-4.47.1-py3-none-any.whl.metadata (44 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.1/44.1 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hINFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. See https://pip.pypa.io/warnings/backtracking for guidance. If you want to abort this run, press Ctrl + C.\n",
            "  Downloading transformers-4.47.0-py3-none-any.whl.metadata (43 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.5/43.5 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading transformers-4.46.3-py3-none-any.whl.metadata (44 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.1/44.1 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.45.1->-r ././requirements/requirements-convert_legacy_llama.txt (line 3)) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.45.1->-r ././requirements/requirements-convert_legacy_llama.txt (line 3)) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.45.1->-r ././requirements/requirements-convert_legacy_llama.txt (line 3)) (2024.11.6)\n",
            "Collecting tokenizers<0.21,>=0.20 (from transformers<5.0.0,>=4.45.1->-r ././requirements/requirements-convert_legacy_llama.txt (line 3))\n",
            "  Downloading tokenizers-0.20.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.45.1->-r ././requirements/requirements-convert_legacy_llama.txt (line 3)) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.45.1->-r ././requirements/requirements-convert_legacy_llama.txt (line 3)) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from torch~=2.2.1->-r ././requirements/requirements-convert_hf_to_gguf.txt (line 3)) (4.13.1)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.11/dist-packages (from torch~=2.2.1->-r ././requirements/requirements-convert_hf_to_gguf.txt (line 3)) (1.13.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch~=2.2.1->-r ././requirements/requirements-convert_hf_to_gguf.txt (line 3)) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch~=2.2.1->-r ././requirements/requirements-convert_hf_to_gguf.txt (line 3)) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch~=2.2.1->-r ././requirements/requirements-convert_hf_to_gguf.txt (line 3)) (2024.12.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp~=3.9.3->-r ././requirements/requirements-tool_bench.txt (line 1)) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp~=3.9.3->-r ././requirements/requirements-tool_bench.txt (line 1)) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp~=3.9.3->-r ././requirements/requirements-tool_bench.txt (line 1)) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp~=3.9.3->-r ././requirements/requirements-tool_bench.txt (line 1)) (6.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp~=3.9.3->-r ././requirements/requirements-tool_bench.txt (line 1)) (1.18.3)\n",
            "Requirement already satisfied: iniconfig in /usr/local/lib/python3.11/dist-packages (from pytest~=8.3.3->-r ././requirements/requirements-tool_bench.txt (line 2)) (2.1.0)\n",
            "Requirement already satisfied: pluggy<2,>=1.5 in /usr/local/lib/python3.11/dist-packages (from pytest~=8.3.3->-r ././requirements/requirements-tool_bench.txt (line 2)) (1.5.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib~=3.10.0->-r ././requirements/requirements-tool_bench.txt (line 4)) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib~=3.10.0->-r ././requirements/requirements-tool_bench.txt (line 4)) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib~=3.10.0->-r ././requirements/requirements-tool_bench.txt (line 4)) (4.57.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib~=3.10.0->-r ././requirements/requirements-tool_bench.txt (line 4)) (1.4.8)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib~=3.10.0->-r ././requirements/requirements-tool_bench.txt (line 4)) (11.1.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib~=3.10.0->-r ././requirements/requirements-tool_bench.txt (line 4)) (3.2.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib~=3.10.0->-r ././requirements/requirements-tool_bench.txt (line 4)) (2.8.2)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from openai~=1.55.3->-r ././requirements/requirements-tool_bench.txt (line 6)) (4.9.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from openai~=1.55.3->-r ././requirements/requirements-tool_bench.txt (line 6)) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from openai~=1.55.3->-r ././requirements/requirements-tool_bench.txt (line 6)) (0.28.1)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from openai~=1.55.3->-r ././requirements/requirements-tool_bench.txt (line 6)) (0.9.0)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.11/dist-packages (from openai~=1.55.3->-r ././requirements/requirements-tool_bench.txt (line 6)) (2.11.2)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from openai~=1.55.3->-r ././requirements/requirements-tool_bench.txt (line 6)) (1.3.1)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas~=2.2.3->-r ././requirements/requirements-tool_bench.txt (line 7)) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas~=2.2.3->-r ././requirements/requirements-tool_bench.txt (line 7)) (2025.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests~=2.32.3->-r ././requirements/requirements-tool_bench.txt (line 9)) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests~=2.32.3->-r ././requirements/requirements-tool_bench.txt (line 9)) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests~=2.32.3->-r ././requirements/requirements-tool_bench.txt (line 9)) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests~=2.32.3->-r ././requirements/requirements-tool_bench.txt (line 9)) (2025.1.31)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from typer~=0.15.1->-r ././requirements/requirements-tool_bench.txt (line 11)) (8.1.8)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer~=0.15.1->-r ././requirements/requirements-tool_bench.txt (line 11)) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.11/dist-packages (from typer~=0.15.1->-r ././requirements/requirements-tool_bench.txt (line 11)) (13.9.4)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->openai~=1.55.3->-r ././requirements/requirements-tool_bench.txt (line 6)) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai~=1.55.3->-r ././requirements/requirements-tool_bench.txt (line 6)) (0.14.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->openai~=1.55.3->-r ././requirements/requirements-tool_bench.txt (line 6)) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.1 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->openai~=1.55.3->-r ././requirements/requirements-tool_bench.txt (line 6)) (2.33.1)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->openai~=1.55.3->-r ././requirements/requirements-tool_bench.txt (line 6)) (0.4.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib~=3.10.0->-r ././requirements/requirements-tool_bench.txt (line 4)) (1.17.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer~=0.15.1->-r ././requirements/requirements-tool_bench.txt (line 11)) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer~=0.15.1->-r ././requirements/requirements-tool_bench.txt (line 11)) (2.18.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from yarl<2.0,>=1.0->aiohttp~=3.9.3->-r ././requirements/requirements-tool_bench.txt (line 1)) (0.3.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch~=2.2.1->-r ././requirements/requirements-convert_hf_to_gguf.txt (line 3)) (3.0.2)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy->torch~=2.2.1->-r ././requirements/requirements-convert_hf_to_gguf.txt (line 3)) (1.3.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer~=0.15.1->-r ././requirements/requirements-tool_bench.txt (line 11)) (0.1.2)\n",
            "Downloading numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.3/18.3 MB\u001b[0m \u001b[31m84.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading transformers-4.46.3-py3-none-any.whl (10.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.0/10.0 MB\u001b[0m \u001b[31m104.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading gguf-0.14.0-py3-none-any.whl (76 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.2/76.2 kB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading protobuf-4.25.6-cp37-abi3-manylinux2014_x86_64.whl (294 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m294.6/294.6 kB\u001b[0m \u001b[31m25.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading aiohttp-3.9.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m62.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading huggingface_hub-0.23.5-py3-none-any.whl (402 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m402.8/402.8 kB\u001b[0m \u001b[31m29.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading openai-1.55.3-py3-none-any.whl (389 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m389.6/389.6 kB\u001b[0m \u001b[31m30.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pandas-2.2.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.1/13.1 MB\u001b[0m \u001b[31m95.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading prometheus_client-0.20.0-py3-none-any.whl (54 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.5/54.5 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tokenizers-0.20.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m91.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: wget\n",
            "  Building wheel for wget (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for wget: filename=wget-3.2-py3-none-any.whl size=9655 sha256=cc47d166ebc997461a5c25520458c4da56d9bb37312b47095fe418579f69d1c8\n",
            "  Stored in directory: /root/.cache/pip/wheels/40/b3/0f/a40dbd1c6861731779f62cc4babcb234387e11d697df70ee97\n",
            "Successfully built wget\n",
            "Installing collected packages: wget, protobuf, prometheus-client, numpy, torch, pandas, huggingface_hub, gguf, aiohttp, tokenizers, openai, transformers\n",
            "  Attempting uninstall: protobuf\n",
            "    Found existing installation: protobuf 3.20.3\n",
            "    Uninstalling protobuf-3.20.3:\n",
            "      Successfully uninstalled protobuf-3.20.3\n",
            "  Attempting uninstall: prometheus-client\n",
            "    Found existing installation: prometheus_client 0.21.1\n",
            "    Uninstalling prometheus_client-0.21.1:\n",
            "      Successfully uninstalled prometheus_client-0.21.1\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 2.0.2\n",
            "    Uninstalling numpy-2.0.2:\n",
            "      Successfully uninstalled numpy-2.0.2\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 2.6.0+cu124\n",
            "    Uninstalling torch-2.6.0+cu124:\n",
            "      Successfully uninstalled torch-2.6.0+cu124\n",
            "  Attempting uninstall: pandas\n",
            "    Found existing installation: pandas 2.2.2\n",
            "    Uninstalling pandas-2.2.2:\n",
            "      Successfully uninstalled pandas-2.2.2\n",
            "  Attempting uninstall: huggingface_hub\n",
            "    Found existing installation: huggingface-hub 0.30.1\n",
            "    Uninstalling huggingface-hub-0.30.1:\n",
            "      Successfully uninstalled huggingface-hub-0.30.1\n",
            "  Attempting uninstall: aiohttp\n",
            "    Found existing installation: aiohttp 3.11.15\n",
            "    Uninstalling aiohttp-3.11.15:\n",
            "      Successfully uninstalled aiohttp-3.11.15\n",
            "  Attempting uninstall: tokenizers\n",
            "    Found existing installation: tokenizers 0.21.1\n",
            "    Uninstalling tokenizers-0.21.1:\n",
            "      Successfully uninstalled tokenizers-0.21.1\n",
            "  Attempting uninstall: openai\n",
            "    Found existing installation: openai 1.70.0\n",
            "    Uninstalling openai-1.70.0:\n",
            "      Successfully uninstalled openai-1.70.0\n",
            "  Attempting uninstall: transformers\n",
            "    Found existing installation: transformers 4.50.3\n",
            "    Uninstalling transformers-4.50.3:\n",
            "      Successfully uninstalled transformers-4.50.3\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "unsloth-zoo 2025.3.17 requires protobuf<4.0.0, but you have protobuf 4.25.6 which is incompatible.\n",
            "datasets 3.5.0 requires huggingface-hub>=0.24.0, but you have huggingface-hub 0.23.5 which is incompatible.\n",
            "xformers 0.0.29.post3 requires torch==2.6.0, but you have torch 2.2.2+cpu which is incompatible.\n",
            "google-colab 1.0.0 requires pandas==2.2.2, but you have pandas 2.2.3 which is incompatible.\n",
            "torchaudio 2.6.0+cu124 requires torch==2.6.0, but you have torch 2.2.2+cpu which is incompatible.\n",
            "torchvision 0.21.0+cu124 requires torch==2.6.0, but you have torch 2.2.2+cpu which is incompatible.\n",
            "ydf 0.11.0 requires protobuf<6.0.0,>=5.29.1, but you have protobuf 4.25.6 which is incompatible.\n",
            "grpcio-status 1.71.0 requires protobuf<6.0dev,>=5.26.1, but you have protobuf 4.25.6 which is incompatible.\n",
            "thinc 8.3.6 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.26.4 which is incompatible.\n",
            "gcsfs 2025.3.2 requires fsspec==2025.3.2, but you have fsspec 2024.12.0 which is incompatible.\n",
            "peft 0.14.0 requires huggingface-hub>=0.25.0, but you have huggingface-hub 0.23.5 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed aiohttp-3.9.5 gguf-0.14.0 huggingface_hub-0.23.5 numpy-1.26.4 openai-1.55.3 pandas-2.2.3 prometheus-client-0.20.0 protobuf-4.25.6 tokenizers-0.20.3 torch-2.2.2+cpu transformers-4.46.3 wget-3.2\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "aiohttp",
                  "functorch",
                  "huggingface_hub",
                  "prometheus_client",
                  "torch",
                  "torchgen",
                  "transformers"
                ]
              },
              "id": "4f3b8de74e1444a39e75e480dafa4e78"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.chdir(\"llama.cpp\")\n",
        "\n",
        "!python3 ./convert_hf_to_gguf.py ../DeepSeek-R1-South-Ostrobothnia --outfile deepseek_south_ostrobothnia.gguf\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nneaL66d9jb5",
        "outputId": "15203302-847b-467b-eaf5-4e0905098fc9"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:hf-to-gguf:Loading model: DeepSeek-R1-South-Ostrobothnia\n",
            "INFO:gguf.gguf_writer:gguf: This GGUF file is for Little Endian only\n",
            "INFO:hf-to-gguf:Exporting model...\n",
            "INFO:hf-to-gguf:rope_freqs.weight,           torch.float32 --> F32, shape = {64}\n",
            "INFO:hf-to-gguf:gguf: loading model weight map from 'pytorch_model.bin.index.json'\n",
            "INFO:hf-to-gguf:gguf: loading model part 'pytorch_model-00001-of-00004.bin'\n",
            "INFO:hf-to-gguf:token_embd.weight,           torch.float16 --> F16, shape = {4096, 128256}\n",
            "INFO:hf-to-gguf:blk.0.attn_q.weight,         torch.float16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.0.attn_k.weight,         torch.float16 --> F16, shape = {4096, 1024}\n",
            "INFO:hf-to-gguf:blk.0.attn_v.weight,         torch.float16 --> F16, shape = {4096, 1024}\n",
            "INFO:hf-to-gguf:blk.0.attn_output.weight,    torch.float16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.0.ffn_gate.weight,       torch.float16 --> F16, shape = {4096, 14336}\n",
            "INFO:hf-to-gguf:blk.0.ffn_up.weight,         torch.float16 --> F16, shape = {4096, 14336}\n",
            "INFO:hf-to-gguf:blk.0.ffn_down.weight,       torch.float16 --> F16, shape = {14336, 4096}\n",
            "INFO:hf-to-gguf:blk.0.attn_norm.weight,      torch.float16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.0.ffn_norm.weight,       torch.float16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.1.attn_q.weight,         torch.float16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.1.attn_k.weight,         torch.float16 --> F16, shape = {4096, 1024}\n",
            "INFO:hf-to-gguf:blk.1.attn_v.weight,         torch.float16 --> F16, shape = {4096, 1024}\n",
            "INFO:hf-to-gguf:blk.1.attn_output.weight,    torch.float16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.1.ffn_gate.weight,       torch.float16 --> F16, shape = {4096, 14336}\n",
            "INFO:hf-to-gguf:blk.1.ffn_up.weight,         torch.float16 --> F16, shape = {4096, 14336}\n",
            "INFO:hf-to-gguf:blk.1.ffn_down.weight,       torch.float16 --> F16, shape = {14336, 4096}\n",
            "INFO:hf-to-gguf:blk.1.attn_norm.weight,      torch.float16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.1.ffn_norm.weight,       torch.float16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.2.attn_q.weight,         torch.float16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.2.attn_k.weight,         torch.float16 --> F16, shape = {4096, 1024}\n",
            "INFO:hf-to-gguf:blk.2.attn_v.weight,         torch.float16 --> F16, shape = {4096, 1024}\n",
            "INFO:hf-to-gguf:blk.2.attn_output.weight,    torch.float16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.2.ffn_gate.weight,       torch.float16 --> F16, shape = {4096, 14336}\n",
            "INFO:hf-to-gguf:blk.2.ffn_up.weight,         torch.float16 --> F16, shape = {4096, 14336}\n",
            "INFO:hf-to-gguf:blk.2.ffn_down.weight,       torch.float16 --> F16, shape = {14336, 4096}\n",
            "INFO:hf-to-gguf:blk.2.attn_norm.weight,      torch.float16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.2.ffn_norm.weight,       torch.float16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.3.attn_q.weight,         torch.float16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.3.attn_k.weight,         torch.float16 --> F16, shape = {4096, 1024}\n",
            "INFO:hf-to-gguf:blk.3.attn_v.weight,         torch.float16 --> F16, shape = {4096, 1024}\n",
            "INFO:hf-to-gguf:blk.3.attn_output.weight,    torch.float16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.3.ffn_gate.weight,       torch.float16 --> F16, shape = {4096, 14336}\n",
            "INFO:hf-to-gguf:blk.3.ffn_up.weight,         torch.float16 --> F16, shape = {4096, 14336}\n",
            "INFO:hf-to-gguf:blk.3.ffn_down.weight,       torch.float16 --> F16, shape = {14336, 4096}\n",
            "INFO:hf-to-gguf:blk.3.attn_norm.weight,      torch.float16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.3.ffn_norm.weight,       torch.float16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.4.attn_q.weight,         torch.float16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.4.attn_k.weight,         torch.float16 --> F16, shape = {4096, 1024}\n",
            "INFO:hf-to-gguf:blk.4.attn_v.weight,         torch.float16 --> F16, shape = {4096, 1024}\n",
            "INFO:hf-to-gguf:blk.4.attn_output.weight,    torch.float16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.4.ffn_gate.weight,       torch.float16 --> F16, shape = {4096, 14336}\n",
            "INFO:hf-to-gguf:blk.4.ffn_up.weight,         torch.float16 --> F16, shape = {4096, 14336}\n",
            "INFO:hf-to-gguf:blk.4.ffn_down.weight,       torch.float16 --> F16, shape = {14336, 4096}\n",
            "INFO:hf-to-gguf:blk.4.attn_norm.weight,      torch.float16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.4.ffn_norm.weight,       torch.float16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.5.attn_q.weight,         torch.float16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.5.attn_k.weight,         torch.float16 --> F16, shape = {4096, 1024}\n",
            "INFO:hf-to-gguf:blk.5.attn_v.weight,         torch.float16 --> F16, shape = {4096, 1024}\n",
            "INFO:hf-to-gguf:blk.5.attn_output.weight,    torch.float16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.5.ffn_gate.weight,       torch.float16 --> F16, shape = {4096, 14336}\n",
            "INFO:hf-to-gguf:blk.5.ffn_up.weight,         torch.float16 --> F16, shape = {4096, 14336}\n",
            "INFO:hf-to-gguf:blk.5.ffn_down.weight,       torch.float16 --> F16, shape = {14336, 4096}\n",
            "INFO:hf-to-gguf:blk.5.attn_norm.weight,      torch.float16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.5.ffn_norm.weight,       torch.float16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.6.attn_q.weight,         torch.float16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.6.attn_k.weight,         torch.float16 --> F16, shape = {4096, 1024}\n",
            "INFO:hf-to-gguf:blk.6.attn_v.weight,         torch.float16 --> F16, shape = {4096, 1024}\n",
            "INFO:hf-to-gguf:blk.6.attn_output.weight,    torch.float16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.6.ffn_gate.weight,       torch.float16 --> F16, shape = {4096, 14336}\n",
            "INFO:hf-to-gguf:blk.6.ffn_up.weight,         torch.float16 --> F16, shape = {4096, 14336}\n",
            "INFO:hf-to-gguf:blk.6.ffn_down.weight,       torch.float16 --> F16, shape = {14336, 4096}\n",
            "INFO:hf-to-gguf:blk.6.attn_norm.weight,      torch.float16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.6.ffn_norm.weight,       torch.float16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.7.attn_q.weight,         torch.float16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.7.attn_k.weight,         torch.float16 --> F16, shape = {4096, 1024}\n",
            "INFO:hf-to-gguf:blk.7.attn_v.weight,         torch.float16 --> F16, shape = {4096, 1024}\n",
            "INFO:hf-to-gguf:blk.7.attn_output.weight,    torch.float16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.7.ffn_gate.weight,       torch.float16 --> F16, shape = {4096, 14336}\n",
            "INFO:hf-to-gguf:blk.7.ffn_up.weight,         torch.float16 --> F16, shape = {4096, 14336}\n",
            "INFO:hf-to-gguf:blk.7.ffn_down.weight,       torch.float16 --> F16, shape = {14336, 4096}\n",
            "INFO:hf-to-gguf:blk.7.attn_norm.weight,      torch.float16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.7.ffn_norm.weight,       torch.float16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.8.attn_q.weight,         torch.float16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.8.attn_k.weight,         torch.float16 --> F16, shape = {4096, 1024}\n",
            "INFO:hf-to-gguf:blk.8.attn_v.weight,         torch.float16 --> F16, shape = {4096, 1024}\n",
            "INFO:hf-to-gguf:blk.8.attn_output.weight,    torch.float16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.8.ffn_gate.weight,       torch.float16 --> F16, shape = {4096, 14336}\n",
            "INFO:hf-to-gguf:blk.8.ffn_up.weight,         torch.float16 --> F16, shape = {4096, 14336}\n",
            "INFO:hf-to-gguf:blk.8.ffn_down.weight,       torch.float16 --> F16, shape = {14336, 4096}\n",
            "INFO:hf-to-gguf:blk.8.attn_norm.weight,      torch.float16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.8.ffn_norm.weight,       torch.float16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:gguf: loading model part 'pytorch_model-00002-of-00004.bin'\n",
            "INFO:hf-to-gguf:blk.9.attn_q.weight,         torch.float16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.9.attn_k.weight,         torch.float16 --> F16, shape = {4096, 1024}\n",
            "INFO:hf-to-gguf:blk.9.attn_v.weight,         torch.float16 --> F16, shape = {4096, 1024}\n",
            "INFO:hf-to-gguf:blk.9.attn_output.weight,    torch.float16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.9.ffn_gate.weight,       torch.float16 --> F16, shape = {4096, 14336}\n",
            "INFO:hf-to-gguf:blk.9.ffn_up.weight,         torch.float16 --> F16, shape = {4096, 14336}\n",
            "INFO:hf-to-gguf:blk.9.ffn_down.weight,       torch.float16 --> F16, shape = {14336, 4096}\n",
            "INFO:hf-to-gguf:blk.9.attn_norm.weight,      torch.float16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.9.ffn_norm.weight,       torch.float16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.10.attn_q.weight,        torch.float16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.10.attn_k.weight,        torch.float16 --> F16, shape = {4096, 1024}\n",
            "INFO:hf-to-gguf:blk.10.attn_v.weight,        torch.float16 --> F16, shape = {4096, 1024}\n",
            "INFO:hf-to-gguf:blk.10.attn_output.weight,   torch.float16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.10.ffn_gate.weight,      torch.float16 --> F16, shape = {4096, 14336}\n",
            "INFO:hf-to-gguf:blk.10.ffn_up.weight,        torch.float16 --> F16, shape = {4096, 14336}\n",
            "INFO:hf-to-gguf:blk.10.ffn_down.weight,      torch.float16 --> F16, shape = {14336, 4096}\n",
            "INFO:hf-to-gguf:blk.10.attn_norm.weight,     torch.float16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.10.ffn_norm.weight,      torch.float16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.11.attn_q.weight,        torch.float16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.11.attn_k.weight,        torch.float16 --> F16, shape = {4096, 1024}\n",
            "INFO:hf-to-gguf:blk.11.attn_v.weight,        torch.float16 --> F16, shape = {4096, 1024}\n",
            "INFO:hf-to-gguf:blk.11.attn_output.weight,   torch.float16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.11.ffn_gate.weight,      torch.float16 --> F16, shape = {4096, 14336}\n",
            "INFO:hf-to-gguf:blk.11.ffn_up.weight,        torch.float16 --> F16, shape = {4096, 14336}\n",
            "INFO:hf-to-gguf:blk.11.ffn_down.weight,      torch.float16 --> F16, shape = {14336, 4096}\n",
            "INFO:hf-to-gguf:blk.11.attn_norm.weight,     torch.float16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.11.ffn_norm.weight,      torch.float16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.12.attn_q.weight,        torch.float16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.12.attn_k.weight,        torch.float16 --> F16, shape = {4096, 1024}\n",
            "INFO:hf-to-gguf:blk.12.attn_v.weight,        torch.float16 --> F16, shape = {4096, 1024}\n",
            "INFO:hf-to-gguf:blk.12.attn_output.weight,   torch.float16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.12.ffn_gate.weight,      torch.float16 --> F16, shape = {4096, 14336}\n",
            "INFO:hf-to-gguf:blk.12.ffn_up.weight,        torch.float16 --> F16, shape = {4096, 14336}\n",
            "INFO:hf-to-gguf:blk.12.ffn_down.weight,      torch.float16 --> F16, shape = {14336, 4096}\n",
            "INFO:hf-to-gguf:blk.12.attn_norm.weight,     torch.float16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.12.ffn_norm.weight,      torch.float16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.13.attn_q.weight,        torch.float16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.13.attn_k.weight,        torch.float16 --> F16, shape = {4096, 1024}\n",
            "INFO:hf-to-gguf:blk.13.attn_v.weight,        torch.float16 --> F16, shape = {4096, 1024}\n",
            "INFO:hf-to-gguf:blk.13.attn_output.weight,   torch.float16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.13.ffn_gate.weight,      torch.float16 --> F16, shape = {4096, 14336}\n",
            "INFO:hf-to-gguf:blk.13.ffn_up.weight,        torch.float16 --> F16, shape = {4096, 14336}\n",
            "INFO:hf-to-gguf:blk.13.ffn_down.weight,      torch.float16 --> F16, shape = {14336, 4096}\n",
            "INFO:hf-to-gguf:blk.13.attn_norm.weight,     torch.float16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.13.ffn_norm.weight,      torch.float16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.14.attn_q.weight,        torch.float16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.14.attn_k.weight,        torch.float16 --> F16, shape = {4096, 1024}\n",
            "INFO:hf-to-gguf:blk.14.attn_v.weight,        torch.float16 --> F16, shape = {4096, 1024}\n",
            "INFO:hf-to-gguf:blk.14.attn_output.weight,   torch.float16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.14.ffn_gate.weight,      torch.float16 --> F16, shape = {4096, 14336}\n",
            "INFO:hf-to-gguf:blk.14.ffn_up.weight,        torch.float16 --> F16, shape = {4096, 14336}\n",
            "INFO:hf-to-gguf:blk.14.ffn_down.weight,      torch.float16 --> F16, shape = {14336, 4096}\n",
            "INFO:hf-to-gguf:blk.14.attn_norm.weight,     torch.float16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.14.ffn_norm.weight,      torch.float16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.15.attn_q.weight,        torch.float16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.15.attn_k.weight,        torch.float16 --> F16, shape = {4096, 1024}\n",
            "INFO:hf-to-gguf:blk.15.attn_v.weight,        torch.float16 --> F16, shape = {4096, 1024}\n",
            "INFO:hf-to-gguf:blk.15.attn_output.weight,   torch.float16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.15.ffn_gate.weight,      torch.float16 --> F16, shape = {4096, 14336}\n",
            "INFO:hf-to-gguf:blk.15.ffn_up.weight,        torch.float16 --> F16, shape = {4096, 14336}\n",
            "INFO:hf-to-gguf:blk.15.ffn_down.weight,      torch.float16 --> F16, shape = {14336, 4096}\n",
            "INFO:hf-to-gguf:blk.15.attn_norm.weight,     torch.float16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.15.ffn_norm.weight,      torch.float16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.16.attn_q.weight,        torch.float16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.16.attn_k.weight,        torch.float16 --> F16, shape = {4096, 1024}\n",
            "INFO:hf-to-gguf:blk.16.attn_v.weight,        torch.float16 --> F16, shape = {4096, 1024}\n",
            "INFO:hf-to-gguf:blk.16.attn_output.weight,   torch.float16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.16.ffn_gate.weight,      torch.float16 --> F16, shape = {4096, 14336}\n",
            "INFO:hf-to-gguf:blk.16.ffn_up.weight,        torch.float16 --> F16, shape = {4096, 14336}\n",
            "INFO:hf-to-gguf:blk.16.ffn_down.weight,      torch.float16 --> F16, shape = {14336, 4096}\n",
            "INFO:hf-to-gguf:blk.16.attn_norm.weight,     torch.float16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.16.ffn_norm.weight,      torch.float16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.17.attn_q.weight,        torch.float16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.17.attn_k.weight,        torch.float16 --> F16, shape = {4096, 1024}\n",
            "INFO:hf-to-gguf:blk.17.attn_v.weight,        torch.float16 --> F16, shape = {4096, 1024}\n",
            "INFO:hf-to-gguf:blk.17.attn_output.weight,   torch.float16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.17.ffn_gate.weight,      torch.float16 --> F16, shape = {4096, 14336}\n",
            "INFO:hf-to-gguf:blk.17.ffn_up.weight,        torch.float16 --> F16, shape = {4096, 14336}\n",
            "INFO:hf-to-gguf:blk.17.ffn_down.weight,      torch.float16 --> F16, shape = {14336, 4096}\n",
            "INFO:hf-to-gguf:blk.17.attn_norm.weight,     torch.float16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.17.ffn_norm.weight,      torch.float16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.18.attn_q.weight,        torch.float16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.18.attn_k.weight,        torch.float16 --> F16, shape = {4096, 1024}\n",
            "INFO:hf-to-gguf:blk.18.attn_v.weight,        torch.float16 --> F16, shape = {4096, 1024}\n",
            "INFO:hf-to-gguf:blk.18.attn_output.weight,   torch.float16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.18.ffn_gate.weight,      torch.float16 --> F16, shape = {4096, 14336}\n",
            "INFO:hf-to-gguf:blk.18.ffn_up.weight,        torch.float16 --> F16, shape = {4096, 14336}\n",
            "INFO:hf-to-gguf:blk.18.ffn_down.weight,      torch.float16 --> F16, shape = {14336, 4096}\n",
            "INFO:hf-to-gguf:blk.18.attn_norm.weight,     torch.float16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.18.ffn_norm.weight,      torch.float16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.19.attn_q.weight,        torch.float16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.19.attn_k.weight,        torch.float16 --> F16, shape = {4096, 1024}\n",
            "INFO:hf-to-gguf:blk.19.attn_v.weight,        torch.float16 --> F16, shape = {4096, 1024}\n",
            "INFO:hf-to-gguf:blk.19.attn_output.weight,   torch.float16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.19.ffn_gate.weight,      torch.float16 --> F16, shape = {4096, 14336}\n",
            "INFO:hf-to-gguf:blk.19.ffn_up.weight,        torch.float16 --> F16, shape = {4096, 14336}\n",
            "INFO:hf-to-gguf:blk.19.ffn_down.weight,      torch.float16 --> F16, shape = {14336, 4096}\n",
            "INFO:hf-to-gguf:blk.19.attn_norm.weight,     torch.float16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.19.ffn_norm.weight,      torch.float16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.20.attn_q.weight,        torch.float16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.20.attn_k.weight,        torch.float16 --> F16, shape = {4096, 1024}\n",
            "INFO:hf-to-gguf:blk.20.attn_v.weight,        torch.float16 --> F16, shape = {4096, 1024}\n",
            "INFO:hf-to-gguf:blk.20.attn_output.weight,   torch.float16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.20.ffn_gate.weight,      torch.float16 --> F16, shape = {4096, 14336}\n",
            "INFO:hf-to-gguf:gguf: loading model part 'pytorch_model-00003-of-00004.bin'\n",
            "INFO:hf-to-gguf:blk.20.ffn_up.weight,        torch.float16 --> F16, shape = {4096, 14336}\n",
            "INFO:hf-to-gguf:blk.20.ffn_down.weight,      torch.float16 --> F16, shape = {14336, 4096}\n",
            "INFO:hf-to-gguf:blk.20.attn_norm.weight,     torch.float16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.20.ffn_norm.weight,      torch.float16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.21.attn_q.weight,        torch.float16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.21.attn_k.weight,        torch.float16 --> F16, shape = {4096, 1024}\n",
            "INFO:hf-to-gguf:blk.21.attn_v.weight,        torch.float16 --> F16, shape = {4096, 1024}\n",
            "INFO:hf-to-gguf:blk.21.attn_output.weight,   torch.float16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.21.ffn_gate.weight,      torch.float16 --> F16, shape = {4096, 14336}\n",
            "INFO:hf-to-gguf:blk.21.ffn_up.weight,        torch.float16 --> F16, shape = {4096, 14336}\n",
            "INFO:hf-to-gguf:blk.21.ffn_down.weight,      torch.float16 --> F16, shape = {14336, 4096}\n",
            "INFO:hf-to-gguf:blk.21.attn_norm.weight,     torch.float16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.21.ffn_norm.weight,      torch.float16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.22.attn_q.weight,        torch.float16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.22.attn_k.weight,        torch.float16 --> F16, shape = {4096, 1024}\n",
            "INFO:hf-to-gguf:blk.22.attn_v.weight,        torch.float16 --> F16, shape = {4096, 1024}\n",
            "INFO:hf-to-gguf:blk.22.attn_output.weight,   torch.float16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.22.ffn_gate.weight,      torch.float16 --> F16, shape = {4096, 14336}\n",
            "INFO:hf-to-gguf:blk.22.ffn_up.weight,        torch.float16 --> F16, shape = {4096, 14336}\n",
            "INFO:hf-to-gguf:blk.22.ffn_down.weight,      torch.float16 --> F16, shape = {14336, 4096}\n",
            "INFO:hf-to-gguf:blk.22.attn_norm.weight,     torch.float16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.22.ffn_norm.weight,      torch.float16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.23.attn_q.weight,        torch.float16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.23.attn_k.weight,        torch.float16 --> F16, shape = {4096, 1024}\n",
            "INFO:hf-to-gguf:blk.23.attn_v.weight,        torch.float16 --> F16, shape = {4096, 1024}\n",
            "INFO:hf-to-gguf:blk.23.attn_output.weight,   torch.float16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.23.ffn_gate.weight,      torch.float16 --> F16, shape = {4096, 14336}\n",
            "INFO:hf-to-gguf:blk.23.ffn_up.weight,        torch.float16 --> F16, shape = {4096, 14336}\n",
            "INFO:hf-to-gguf:blk.23.ffn_down.weight,      torch.float16 --> F16, shape = {14336, 4096}\n",
            "INFO:hf-to-gguf:blk.23.attn_norm.weight,     torch.float16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.23.ffn_norm.weight,      torch.float16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.24.attn_q.weight,        torch.float16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.24.attn_k.weight,        torch.float16 --> F16, shape = {4096, 1024}\n",
            "INFO:hf-to-gguf:blk.24.attn_v.weight,        torch.float16 --> F16, shape = {4096, 1024}\n",
            "INFO:hf-to-gguf:blk.24.attn_output.weight,   torch.float16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.24.ffn_gate.weight,      torch.float16 --> F16, shape = {4096, 14336}\n",
            "INFO:hf-to-gguf:blk.24.ffn_up.weight,        torch.float16 --> F16, shape = {4096, 14336}\n",
            "INFO:hf-to-gguf:blk.24.ffn_down.weight,      torch.float16 --> F16, shape = {14336, 4096}\n",
            "INFO:hf-to-gguf:blk.24.attn_norm.weight,     torch.float16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.24.ffn_norm.weight,      torch.float16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.25.attn_q.weight,        torch.float16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.25.attn_k.weight,        torch.float16 --> F16, shape = {4096, 1024}\n",
            "INFO:hf-to-gguf:blk.25.attn_v.weight,        torch.float16 --> F16, shape = {4096, 1024}\n",
            "INFO:hf-to-gguf:blk.25.attn_output.weight,   torch.float16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.25.ffn_gate.weight,      torch.float16 --> F16, shape = {4096, 14336}\n",
            "INFO:hf-to-gguf:blk.25.ffn_up.weight,        torch.float16 --> F16, shape = {4096, 14336}\n",
            "INFO:hf-to-gguf:blk.25.ffn_down.weight,      torch.float16 --> F16, shape = {14336, 4096}\n",
            "INFO:hf-to-gguf:blk.25.attn_norm.weight,     torch.float16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.25.ffn_norm.weight,      torch.float16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.26.attn_q.weight,        torch.float16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.26.attn_k.weight,        torch.float16 --> F16, shape = {4096, 1024}\n",
            "INFO:hf-to-gguf:blk.26.attn_v.weight,        torch.float16 --> F16, shape = {4096, 1024}\n",
            "INFO:hf-to-gguf:blk.26.attn_output.weight,   torch.float16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.26.ffn_gate.weight,      torch.float16 --> F16, shape = {4096, 14336}\n",
            "INFO:hf-to-gguf:blk.26.ffn_up.weight,        torch.float16 --> F16, shape = {4096, 14336}\n",
            "INFO:hf-to-gguf:blk.26.ffn_down.weight,      torch.float16 --> F16, shape = {14336, 4096}\n",
            "INFO:hf-to-gguf:blk.26.attn_norm.weight,     torch.float16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.26.ffn_norm.weight,      torch.float16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.27.attn_q.weight,        torch.float16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.27.attn_k.weight,        torch.float16 --> F16, shape = {4096, 1024}\n",
            "INFO:hf-to-gguf:blk.27.attn_v.weight,        torch.float16 --> F16, shape = {4096, 1024}\n",
            "INFO:hf-to-gguf:blk.27.attn_output.weight,   torch.float16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.27.ffn_gate.weight,      torch.float16 --> F16, shape = {4096, 14336}\n",
            "INFO:hf-to-gguf:blk.27.ffn_up.weight,        torch.float16 --> F16, shape = {4096, 14336}\n",
            "INFO:hf-to-gguf:blk.27.ffn_down.weight,      torch.float16 --> F16, shape = {14336, 4096}\n",
            "INFO:hf-to-gguf:blk.27.attn_norm.weight,     torch.float16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.27.ffn_norm.weight,      torch.float16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.28.attn_q.weight,        torch.float16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.28.attn_k.weight,        torch.float16 --> F16, shape = {4096, 1024}\n",
            "INFO:hf-to-gguf:blk.28.attn_v.weight,        torch.float16 --> F16, shape = {4096, 1024}\n",
            "INFO:hf-to-gguf:blk.28.attn_output.weight,   torch.float16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.28.ffn_gate.weight,      torch.float16 --> F16, shape = {4096, 14336}\n",
            "INFO:hf-to-gguf:blk.28.ffn_up.weight,        torch.float16 --> F16, shape = {4096, 14336}\n",
            "INFO:hf-to-gguf:blk.28.ffn_down.weight,      torch.float16 --> F16, shape = {14336, 4096}\n",
            "INFO:hf-to-gguf:blk.28.attn_norm.weight,     torch.float16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.28.ffn_norm.weight,      torch.float16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.29.attn_q.weight,        torch.float16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.29.attn_k.weight,        torch.float16 --> F16, shape = {4096, 1024}\n",
            "INFO:hf-to-gguf:blk.29.attn_v.weight,        torch.float16 --> F16, shape = {4096, 1024}\n",
            "INFO:hf-to-gguf:blk.29.attn_output.weight,   torch.float16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.29.ffn_gate.weight,      torch.float16 --> F16, shape = {4096, 14336}\n",
            "INFO:hf-to-gguf:blk.29.ffn_up.weight,        torch.float16 --> F16, shape = {4096, 14336}\n",
            "INFO:hf-to-gguf:blk.29.ffn_down.weight,      torch.float16 --> F16, shape = {14336, 4096}\n",
            "INFO:hf-to-gguf:blk.29.attn_norm.weight,     torch.float16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.29.ffn_norm.weight,      torch.float16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.30.attn_q.weight,        torch.float16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.30.attn_k.weight,        torch.float16 --> F16, shape = {4096, 1024}\n",
            "INFO:hf-to-gguf:blk.30.attn_v.weight,        torch.float16 --> F16, shape = {4096, 1024}\n",
            "INFO:hf-to-gguf:blk.30.attn_output.weight,   torch.float16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.30.ffn_gate.weight,      torch.float16 --> F16, shape = {4096, 14336}\n",
            "INFO:hf-to-gguf:blk.30.ffn_up.weight,        torch.float16 --> F16, shape = {4096, 14336}\n",
            "INFO:hf-to-gguf:blk.30.ffn_down.weight,      torch.float16 --> F16, shape = {14336, 4096}\n",
            "INFO:hf-to-gguf:blk.30.attn_norm.weight,     torch.float16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.30.ffn_norm.weight,      torch.float16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.31.attn_q.weight,        torch.float16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.31.attn_k.weight,        torch.float16 --> F16, shape = {4096, 1024}\n",
            "INFO:hf-to-gguf:blk.31.attn_v.weight,        torch.float16 --> F16, shape = {4096, 1024}\n",
            "INFO:hf-to-gguf:blk.31.attn_output.weight,   torch.float16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.31.ffn_gate.weight,      torch.float16 --> F16, shape = {4096, 14336}\n",
            "INFO:hf-to-gguf:blk.31.ffn_up.weight,        torch.float16 --> F16, shape = {4096, 14336}\n",
            "INFO:hf-to-gguf:gguf: loading model part 'pytorch_model-00004-of-00004.bin'\n",
            "INFO:hf-to-gguf:blk.31.ffn_down.weight,      torch.float16 --> F16, shape = {14336, 4096}\n",
            "INFO:hf-to-gguf:blk.31.attn_norm.weight,     torch.float16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.31.ffn_norm.weight,      torch.float16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:output_norm.weight,          torch.float16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:output.weight,               torch.float16 --> F16, shape = {4096, 128256}\n",
            "INFO:hf-to-gguf:Set meta model\n",
            "INFO:hf-to-gguf:Set model parameters\n",
            "INFO:hf-to-gguf:gguf: context length = 131072\n",
            "INFO:hf-to-gguf:gguf: embedding length = 4096\n",
            "INFO:hf-to-gguf:gguf: feed forward length = 14336\n",
            "INFO:hf-to-gguf:gguf: head count = 32\n",
            "INFO:hf-to-gguf:gguf: key-value head count = 8\n",
            "INFO:hf-to-gguf:gguf: rope theta = 500000.0\n",
            "INFO:hf-to-gguf:gguf: rms norm epsilon = 1e-05\n",
            "INFO:hf-to-gguf:gguf: file type = 1\n",
            "INFO:hf-to-gguf:Set model tokenizer\n",
            "The cache for model files in Transformers v4.22.0 has been updated. Migrating your old cache. This is a one-time only operation. You can interrupt this and resume the migration later on by calling `transformers.utils.move_cache()`.\n",
            "0it [00:00, ?it/s]\n",
            "2025-04-13 17:24:16.144265: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1744565056.240070   17321 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1744565056.261408   17321 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2025-04-13 17:24:16.339554: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "INFO:numexpr.utils:NumExpr defaulting to 2 threads.\n",
            "INFO:gguf.vocab:Adding 280147 merge(s).\n",
            "INFO:gguf.vocab:Setting special token type bos to 128000\n",
            "INFO:gguf.vocab:Setting special token type eos to 128001\n",
            "INFO:gguf.vocab:Setting special token type pad to 128004\n",
            "INFO:gguf.vocab:Setting add_bos_token to True\n",
            "INFO:gguf.vocab:Setting add_eos_token to False\n",
            "INFO:gguf.vocab:Setting chat_template to {% if not add_generation_prompt is defined %}{% set add_generation_prompt = false %}{% endif %}{% set ns = namespace(is_first=false, is_tool=false, is_output_first=true, system_prompt='') %}{%- for message in messages %}{%- if message['role'] == 'system' %}{% set ns.system_prompt = message['content'] %}{%- endif %}{%- endfor %}{{bos_token}}{{ns.system_prompt}}{%- for message in messages %}{%- if message['role'] == 'user' %}{%- set ns.is_tool = false -%}{{'<｜User｜>' + message['content']}}{%- endif %}{%- if message['role'] == 'assistant' and message['content'] is none %}{%- set ns.is_tool = false -%}{%- for tool in message['tool_calls']%}{%- if not ns.is_first %}{{'<｜Assistant｜><｜tool▁calls▁begin｜><｜tool▁call▁begin｜>' + tool['type'] + '<｜tool▁sep｜>' + tool['function']['name'] + '\\n' + '```json' + '\\n' + tool['function']['arguments'] + '\\n' + '```' + '<｜tool▁call▁end｜>'}}{%- set ns.is_first = true -%}{%- else %}{{'\\n' + '<｜tool▁call▁begin｜>' + tool['type'] + '<｜tool▁sep｜>' + tool['function']['name'] + '\\n' + '```json' + '\\n' + tool['function']['arguments'] + '\\n' + '```' + '<｜tool▁call▁end｜>'}}{{'<｜tool▁calls▁end｜><｜end▁of▁sentence｜>'}}{%- endif %}{%- endfor %}{%- endif %}{%- if message['role'] == 'assistant' and message['content'] is not none %}{%- if ns.is_tool %}{{'<｜tool▁outputs▁end｜>' + message['content'] + '<｜end▁of▁sentence｜>'}}{%- set ns.is_tool = false -%}{%- else %}{% set content = message['content'] %}{% if '</think>' in content %}{% set content = content.split('</think>')[-1] %}{% endif %}{{'<｜Assistant｜>' + content + '<｜end▁of▁sentence｜>'}}{%- endif %}{%- endif %}{%- if message['role'] == 'tool' %}{%- set ns.is_tool = true -%}{%- if ns.is_output_first %}{{'<｜tool▁outputs▁begin｜><｜tool▁output▁begin｜>' + message['content'] + '<｜tool▁output▁end｜>'}}{%- set ns.is_output_first = false %}{%- else %}{{'\\n<｜tool▁output▁begin｜>' + message['content'] + '<｜tool▁output▁end｜>'}}{%- endif %}{%- endif %}{%- endfor -%}{% if ns.is_tool %}{{'<｜tool▁outputs▁end｜>'}}{% endif %}{% if add_generation_prompt and not ns.is_tool %}{{'<｜Assistant｜><think>\\n'}}{% endif %}\n",
            "INFO:hf-to-gguf:Set model quantization version\n",
            "INFO:gguf.gguf_writer:Writing the following files:\n",
            "INFO:gguf.gguf_writer:deepseek_south_ostrobothnia.gguf: n_tensors = 292, total_size = 16.1G\n",
            "Writing: 100% 16.1G/16.1G [03:12<00:00, 83.6Mbyte/s]\n",
            "INFO:hf-to-gguf:Model successfully exported to deepseek_south_ostrobothnia.gguf\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#os.makedirs(\"build\",exist_ok=True)\n",
        "os.chdir(\"build\")\n",
        "!cmake ..\n",
        "!cmake --build . --config Release"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v8UhZZwWMkqB",
        "outputId": "430d62c4-f339-4e3e-8078-0726a9e36db9"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-- The C compiler identification is GNU 11.4.0\n",
            "-- The CXX compiler identification is GNU 11.4.0\n",
            "-- Detecting C compiler ABI info\n",
            "-- Detecting C compiler ABI info - done\n",
            "-- Check for working C compiler: /usr/bin/cc - skipped\n",
            "-- Detecting C compile features\n",
            "-- Detecting C compile features - done\n",
            "-- Detecting CXX compiler ABI info\n",
            "-- Detecting CXX compiler ABI info - done\n",
            "-- Check for working CXX compiler: /usr/bin/c++ - skipped\n",
            "-- Detecting CXX compile features\n",
            "-- Detecting CXX compile features - done\n",
            "-- Found Git: /usr/bin/git (found version \"2.34.1\")\n",
            "-- Performing Test CMAKE_HAVE_LIBC_PTHREAD\n",
            "-- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Success\n",
            "-- Found Threads: TRUE\n",
            "-- Warning: ccache not found - consider installing it for faster compilation or disable this warning with GGML_CCACHE=OFF\n",
            "-- CMAKE_SYSTEM_PROCESSOR: x86_64\n",
            "-- Including CPU backend\n",
            "-- Found OpenMP_C: -fopenmp (found version \"4.5\")\n",
            "-- Found OpenMP_CXX: -fopenmp (found version \"4.5\")\n",
            "-- Found OpenMP: TRUE (found version \"4.5\")\n",
            "-- x86 detected\n",
            "-- Adding CPU backend variant ggml-cpu: -march=native \n",
            "-- Found CURL: /usr/lib/x86_64-linux-gnu/libcurl.so (found version \"7.81.0\")\n",
            "-- Configuring done (2.4s)\n",
            "-- Generating done (0.4s)\n",
            "-- Build files have been written to: /content/llama.cpp/build\n",
            "[  1%] \u001b[32mBuilding C object ggml/src/CMakeFiles/ggml-base.dir/ggml.c.o\u001b[0m\n",
            "[  1%] \u001b[32mBuilding C object ggml/src/CMakeFiles/ggml-base.dir/ggml-alloc.c.o\u001b[0m\n",
            "[  2%] \u001b[32mBuilding CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-backend.cpp.o\u001b[0m\n",
            "[  2%] \u001b[32mBuilding CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-opt.cpp.o\u001b[0m\n",
            "[  3%] \u001b[32mBuilding CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-threading.cpp.o\u001b[0m\n",
            "[  3%] \u001b[32mBuilding C object ggml/src/CMakeFiles/ggml-base.dir/ggml-quants.c.o\u001b[0m\n",
            "[  4%] \u001b[32mBuilding CXX object ggml/src/CMakeFiles/ggml-base.dir/gguf.cpp.o\u001b[0m\n",
            "[  4%] \u001b[32m\u001b[1mLinking CXX shared library ../../bin/libggml-base.so\u001b[0m\n",
            "[  4%] Built target ggml-base\n",
            "[  4%] \u001b[32mBuilding C object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu.c.o\u001b[0m\n",
            "[  5%] \u001b[32mBuilding CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu.cpp.o\u001b[0m\n",
            "[  5%] \u001b[32mBuilding CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-aarch64.cpp.o\u001b[0m\n",
            "[  6%] \u001b[32mBuilding CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-hbm.cpp.o\u001b[0m\n",
            "[  6%] \u001b[32mBuilding C object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-quants.c.o\u001b[0m\n",
            "[  7%] \u001b[32mBuilding CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-traits.cpp.o\u001b[0m\n",
            "[  7%] \u001b[32mBuilding CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/amx/amx.cpp.o\u001b[0m\n",
            "[  8%] \u001b[32mBuilding CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/amx/mmq.cpp.o\u001b[0m\n",
            "[  8%] \u001b[32mBuilding CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/binary-ops.cpp.o\u001b[0m\n",
            "[  9%] \u001b[32mBuilding CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/unary-ops.cpp.o\u001b[0m\n",
            "[  9%] \u001b[32mBuilding CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/vec.cpp.o\u001b[0m\n",
            "[  9%] \u001b[32mBuilding CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ops.cpp.o\u001b[0m\n",
            "[ 10%] \u001b[32mBuilding CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/llamafile/sgemm.cpp.o\u001b[0m\n",
            "[ 10%] \u001b[32m\u001b[1mLinking CXX shared library ../../bin/libggml-cpu.so\u001b[0m\n",
            "[ 10%] Built target ggml-cpu\n",
            "[ 11%] \u001b[32mBuilding CXX object ggml/src/CMakeFiles/ggml.dir/ggml-backend-reg.cpp.o\u001b[0m\n",
            "[ 11%] \u001b[32m\u001b[1mLinking CXX shared library ../../bin/libggml.so\u001b[0m\n",
            "[ 11%] Built target ggml\n",
            "[ 12%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama.cpp.o\u001b[0m\n",
            "[ 12%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-adapter.cpp.o\u001b[0m\n",
            "[ 13%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-arch.cpp.o\u001b[0m\n",
            "[ 13%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-batch.cpp.o\u001b[0m\n",
            "[ 14%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-chat.cpp.o\u001b[0m\n",
            "[ 14%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-context.cpp.o\u001b[0m\n",
            "[ 15%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-grammar.cpp.o\u001b[0m\n",
            "[ 15%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-graph.cpp.o\u001b[0m\n",
            "[ 15%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-hparams.cpp.o\u001b[0m\n",
            "[ 16%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-impl.cpp.o\u001b[0m\n",
            "[ 16%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-io.cpp.o\u001b[0m\n",
            "[ 17%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-kv-cache.cpp.o\u001b[0m\n",
            "[ 17%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-memory.cpp.o\u001b[0m\n",
            "[ 18%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-mmap.cpp.o\u001b[0m\n",
            "[ 18%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-model-loader.cpp.o\u001b[0m\n",
            "[ 19%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-model.cpp.o\u001b[0m\n",
            "[ 19%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-quant.cpp.o\u001b[0m\n",
            "[ 20%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-sampling.cpp.o\u001b[0m\n",
            "[ 20%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-vocab.cpp.o\u001b[0m\n",
            "[ 20%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/unicode-data.cpp.o\u001b[0m\n",
            "[ 21%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/unicode.cpp.o\u001b[0m\n",
            "[ 21%] \u001b[32m\u001b[1mLinking CXX shared library ../bin/libllama.so\u001b[0m\n",
            "[ 21%] Built target llama\n",
            "[ 21%] \u001b[34m\u001b[1mGenerating build details from Git\u001b[0m\n",
            "-- Found Git: /usr/bin/git (found version \"2.34.1\")\n",
            "[ 21%] \u001b[32mBuilding CXX object common/CMakeFiles/build_info.dir/build-info.cpp.o\u001b[0m\n",
            "[ 21%] Built target build_info\n",
            "[ 22%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/arg.cpp.o\u001b[0m\n",
            "[ 22%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/chat.cpp.o\u001b[0m\n",
            "[ 23%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/common.cpp.o\u001b[0m\n",
            "[ 23%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/console.cpp.o\u001b[0m\n",
            "[ 24%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/json-schema-to-grammar.cpp.o\u001b[0m\n",
            "[ 24%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/llguidance.cpp.o\u001b[0m\n",
            "[ 25%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/log.cpp.o\u001b[0m\n",
            "[ 25%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/ngram-cache.cpp.o\u001b[0m\n",
            "[ 26%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/sampling.cpp.o\u001b[0m\n",
            "[ 26%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/speculative.cpp.o\u001b[0m\n",
            "[ 26%] \u001b[32m\u001b[1mLinking CXX static library libcommon.a\u001b[0m\n",
            "[ 26%] Built target common\n",
            "[ 27%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-tokenizer-0.dir/test-tokenizer-0.cpp.o\u001b[0m\n",
            "[ 27%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-tokenizer-0\u001b[0m\n",
            "[ 27%] Built target test-tokenizer-0\n",
            "[ 27%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-sampling.dir/test-sampling.cpp.o\u001b[0m\n",
            "[ 28%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-sampling.dir/get-model.cpp.o\u001b[0m\n",
            "[ 28%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-sampling\u001b[0m\n",
            "[ 28%] Built target test-sampling\n",
            "[ 29%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-grammar-parser.dir/test-grammar-parser.cpp.o\u001b[0m\n",
            "[ 29%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-grammar-parser.dir/get-model.cpp.o\u001b[0m\n",
            "[ 29%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-grammar-parser\u001b[0m\n",
            "[ 29%] Built target test-grammar-parser\n",
            "[ 29%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-grammar-integration.dir/test-grammar-integration.cpp.o\u001b[0m\n",
            "[ 30%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-grammar-integration.dir/get-model.cpp.o\u001b[0m\n",
            "[ 30%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-grammar-integration\u001b[0m\n",
            "[ 30%] Built target test-grammar-integration\n",
            "[ 30%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-llama-grammar.dir/test-llama-grammar.cpp.o\u001b[0m\n",
            "[ 31%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-llama-grammar.dir/get-model.cpp.o\u001b[0m\n",
            "[ 31%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-llama-grammar\u001b[0m\n",
            "[ 31%] Built target test-llama-grammar\n",
            "[ 31%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-chat.dir/test-chat.cpp.o\u001b[0m\n",
            "[ 32%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-chat.dir/get-model.cpp.o\u001b[0m\n",
            "[ 32%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-chat\u001b[0m\n",
            "[ 32%] Built target test-chat\n",
            "[ 33%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-json-schema-to-grammar.dir/test-json-schema-to-grammar.cpp.o\u001b[0m\n",
            "[ 33%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-json-schema-to-grammar.dir/get-model.cpp.o\u001b[0m\n",
            "[ 34%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-json-schema-to-grammar\u001b[0m\n",
            "[ 34%] Built target test-json-schema-to-grammar\n",
            "[ 35%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-tokenizer-1-bpe.dir/test-tokenizer-1-bpe.cpp.o\u001b[0m\n",
            "[ 35%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-tokenizer-1-bpe\u001b[0m\n",
            "[ 35%] Built target test-tokenizer-1-bpe\n",
            "[ 36%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-tokenizer-1-spm.dir/test-tokenizer-1-spm.cpp.o\u001b[0m\n",
            "[ 36%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-tokenizer-1-spm\u001b[0m\n",
            "[ 36%] Built target test-tokenizer-1-spm\n",
            "[ 37%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-log.dir/test-log.cpp.o\u001b[0m\n",
            "[ 37%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-log.dir/get-model.cpp.o\u001b[0m\n",
            "[ 38%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-log\u001b[0m\n",
            "[ 38%] Built target test-log\n",
            "[ 38%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-chat-template.dir/test-chat-template.cpp.o\u001b[0m\n",
            "[ 39%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-chat-template.dir/get-model.cpp.o\u001b[0m\n",
            "[ 39%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-chat-template\u001b[0m\n",
            "[ 39%] Built target test-chat-template\n",
            "[ 40%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-arg-parser.dir/test-arg-parser.cpp.o\u001b[0m\n",
            "[ 40%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-arg-parser.dir/get-model.cpp.o\u001b[0m\n",
            "[ 41%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-arg-parser\u001b[0m\n",
            "[ 41%] Built target test-arg-parser\n",
            "[ 42%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-gguf.dir/test-gguf.cpp.o\u001b[0m\n",
            "[ 42%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-gguf.dir/get-model.cpp.o\u001b[0m\n",
            "[ 43%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-gguf\u001b[0m\n",
            "[ 43%] Built target test-gguf\n",
            "[ 43%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-backend-ops.dir/test-backend-ops.cpp.o\u001b[0m\n",
            "[ 44%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-backend-ops.dir/get-model.cpp.o\u001b[0m\n",
            "[ 44%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-backend-ops\u001b[0m\n",
            "[ 44%] Built target test-backend-ops\n",
            "[ 44%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-model-load-cancel.dir/test-model-load-cancel.cpp.o\u001b[0m\n",
            "[ 44%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-model-load-cancel.dir/get-model.cpp.o\u001b[0m\n",
            "[ 45%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-model-load-cancel\u001b[0m\n",
            "[ 45%] Built target test-model-load-cancel\n",
            "[ 45%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-autorelease.dir/test-autorelease.cpp.o\u001b[0m\n",
            "[ 46%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-autorelease.dir/get-model.cpp.o\u001b[0m\n",
            "[ 46%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-autorelease\u001b[0m\n",
            "[ 46%] Built target test-autorelease\n",
            "[ 47%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-barrier.dir/test-barrier.cpp.o\u001b[0m\n",
            "[ 47%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-barrier.dir/get-model.cpp.o\u001b[0m\n",
            "[ 48%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-barrier\u001b[0m\n",
            "[ 48%] Built target test-barrier\n",
            "[ 48%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-quantize-fns.dir/test-quantize-fns.cpp.o\u001b[0m\n",
            "[ 49%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-quantize-fns.dir/get-model.cpp.o\u001b[0m\n",
            "[ 49%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-quantize-fns\u001b[0m\n",
            "[ 49%] Built target test-quantize-fns\n",
            "[ 50%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-quantize-perf.dir/test-quantize-perf.cpp.o\u001b[0m\n",
            "[ 50%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-quantize-perf.dir/get-model.cpp.o\u001b[0m\n",
            "[ 51%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-quantize-perf\u001b[0m\n",
            "[ 51%] Built target test-quantize-perf\n",
            "[ 51%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-rope.dir/test-rope.cpp.o\u001b[0m\n",
            "[ 52%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-rope.dir/get-model.cpp.o\u001b[0m\n",
            "[ 52%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-rope\u001b[0m\n",
            "[ 52%] Built target test-rope\n",
            "[ 52%] \u001b[32mBuilding C object tests/CMakeFiles/test-c.dir/test-c.c.o\u001b[0m\n",
            "[ 53%] \u001b[32m\u001b[1mLinking C executable ../bin/test-c\u001b[0m\n",
            "[ 53%] Built target test-c\n",
            "[ 54%] \u001b[32mBuilding CXX object examples/batched-bench/CMakeFiles/llama-batched-bench.dir/batched-bench.cpp.o\u001b[0m\n",
            "[ 54%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-batched-bench\u001b[0m\n",
            "[ 54%] Built target llama-batched-bench\n",
            "[ 55%] \u001b[32mBuilding CXX object examples/batched/CMakeFiles/llama-batched.dir/batched.cpp.o\u001b[0m\n",
            "[ 55%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-batched\u001b[0m\n",
            "[ 55%] Built target llama-batched\n",
            "[ 55%] \u001b[32mBuilding CXX object examples/embedding/CMakeFiles/llama-embedding.dir/embedding.cpp.o\u001b[0m\n",
            "[ 56%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-embedding\u001b[0m\n",
            "[ 56%] Built target llama-embedding\n",
            "[ 56%] \u001b[32mBuilding CXX object examples/eval-callback/CMakeFiles/llama-eval-callback.dir/eval-callback.cpp.o\u001b[0m\n",
            "[ 57%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-eval-callback\u001b[0m\n",
            "[ 57%] Built target llama-eval-callback\n",
            "[ 57%] \u001b[32mBuilding CXX object examples/gbnf-validator/CMakeFiles/llama-gbnf-validator.dir/gbnf-validator.cpp.o\u001b[0m\n",
            "[ 57%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-gbnf-validator\u001b[0m\n",
            "[ 57%] Built target llama-gbnf-validator\n",
            "[ 57%] \u001b[32mBuilding C object examples/gguf-hash/CMakeFiles/sha256.dir/deps/sha256/sha256.c.o\u001b[0m\n",
            "[ 57%] Built target sha256\n",
            "[ 58%] \u001b[32mBuilding C object examples/gguf-hash/CMakeFiles/xxhash.dir/deps/xxhash/xxhash.c.o\u001b[0m\n",
            "[ 58%] Built target xxhash\n",
            "[ 59%] \u001b[32mBuilding C object examples/gguf-hash/CMakeFiles/sha1.dir/deps/sha1/sha1.c.o\u001b[0m\n",
            "[ 59%] Built target sha1\n",
            "[ 60%] \u001b[32mBuilding CXX object examples/gguf-hash/CMakeFiles/llama-gguf-hash.dir/gguf-hash.cpp.o\u001b[0m\n",
            "[ 60%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-gguf-hash\u001b[0m\n",
            "[ 60%] Built target llama-gguf-hash\n",
            "[ 61%] \u001b[32mBuilding CXX object examples/gguf-split/CMakeFiles/llama-gguf-split.dir/gguf-split.cpp.o\u001b[0m\n",
            "[ 61%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-gguf-split\u001b[0m\n",
            "[ 61%] Built target llama-gguf-split\n",
            "[ 62%] \u001b[32mBuilding CXX object examples/gguf/CMakeFiles/llama-gguf.dir/gguf.cpp.o\u001b[0m\n",
            "[ 62%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-gguf\u001b[0m\n",
            "[ 62%] Built target llama-gguf\n",
            "[ 62%] \u001b[32mBuilding CXX object examples/gritlm/CMakeFiles/llama-gritlm.dir/gritlm.cpp.o\u001b[0m\n",
            "[ 63%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-gritlm\u001b[0m\n",
            "[ 63%] Built target llama-gritlm\n",
            "[ 63%] \u001b[32mBuilding CXX object examples/imatrix/CMakeFiles/llama-imatrix.dir/imatrix.cpp.o\u001b[0m\n",
            "[ 64%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-imatrix\u001b[0m\n",
            "[ 64%] Built target llama-imatrix\n",
            "[ 64%] \u001b[32mBuilding CXX object examples/infill/CMakeFiles/llama-infill.dir/infill.cpp.o\u001b[0m\n",
            "[ 65%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-infill\u001b[0m\n",
            "[ 65%] Built target llama-infill\n",
            "[ 66%] \u001b[32mBuilding CXX object examples/llama-bench/CMakeFiles/llama-bench.dir/llama-bench.cpp.o\u001b[0m\n",
            "[ 66%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-bench\u001b[0m\n",
            "[ 66%] Built target llama-bench\n",
            "[ 66%] \u001b[32mBuilding CXX object examples/lookahead/CMakeFiles/llama-lookahead.dir/lookahead.cpp.o\u001b[0m\n",
            "[ 66%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-lookahead\u001b[0m\n",
            "[ 66%] Built target llama-lookahead\n",
            "[ 67%] \u001b[32mBuilding CXX object examples/lookup/CMakeFiles/llama-lookup.dir/lookup.cpp.o\u001b[0m\n",
            "[ 67%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-lookup\u001b[0m\n",
            "[ 67%] Built target llama-lookup\n",
            "[ 68%] \u001b[32mBuilding CXX object examples/lookup/CMakeFiles/llama-lookup-create.dir/lookup-create.cpp.o\u001b[0m\n",
            "[ 68%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-lookup-create\u001b[0m\n",
            "[ 68%] Built target llama-lookup-create\n",
            "[ 69%] \u001b[32mBuilding CXX object examples/lookup/CMakeFiles/llama-lookup-merge.dir/lookup-merge.cpp.o\u001b[0m\n",
            "[ 69%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-lookup-merge\u001b[0m\n",
            "[ 69%] Built target llama-lookup-merge\n",
            "[ 70%] \u001b[32mBuilding CXX object examples/lookup/CMakeFiles/llama-lookup-stats.dir/lookup-stats.cpp.o\u001b[0m\n",
            "[ 70%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-lookup-stats\u001b[0m\n",
            "[ 70%] Built target llama-lookup-stats\n",
            "[ 71%] \u001b[32mBuilding CXX object examples/main/CMakeFiles/llama-cli.dir/main.cpp.o\u001b[0m\n",
            "[ 71%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-cli\u001b[0m\n",
            "[ 71%] Built target llama-cli\n",
            "[ 71%] \u001b[32mBuilding CXX object examples/parallel/CMakeFiles/llama-parallel.dir/parallel.cpp.o\u001b[0m\n",
            "[ 72%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-parallel\u001b[0m\n",
            "[ 72%] Built target llama-parallel\n",
            "[ 72%] \u001b[32mBuilding CXX object examples/passkey/CMakeFiles/llama-passkey.dir/passkey.cpp.o\u001b[0m\n",
            "[ 73%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-passkey\u001b[0m\n",
            "[ 73%] Built target llama-passkey\n",
            "[ 73%] \u001b[32mBuilding CXX object examples/perplexity/CMakeFiles/llama-perplexity.dir/perplexity.cpp.o\u001b[0m\n",
            "[ 74%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-perplexity\u001b[0m\n",
            "[ 74%] Built target llama-perplexity\n",
            "[ 74%] \u001b[32mBuilding CXX object examples/quantize/CMakeFiles/llama-quantize.dir/quantize.cpp.o\u001b[0m\n",
            "[ 75%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-quantize\u001b[0m\n",
            "[ 75%] Built target llama-quantize\n",
            "[ 76%] \u001b[32mBuilding CXX object examples/retrieval/CMakeFiles/llama-retrieval.dir/retrieval.cpp.o\u001b[0m\n",
            "[ 76%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-retrieval\u001b[0m\n",
            "[ 76%] Built target llama-retrieval\n",
            "[ 76%] \u001b[34m\u001b[1mGenerating loading.html.hpp\u001b[0m\n",
            "[ 76%] \u001b[34m\u001b[1mGenerating index.html.gz.hpp\u001b[0m\n",
            "[ 77%] \u001b[32mBuilding CXX object examples/server/CMakeFiles/llama-server.dir/server.cpp.o\u001b[0m\n",
            "[ 77%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-server\u001b[0m\n",
            "[ 77%] Built target llama-server\n",
            "[ 77%] \u001b[32mBuilding CXX object examples/save-load-state/CMakeFiles/llama-save-load-state.dir/save-load-state.cpp.o\u001b[0m\n",
            "[ 78%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-save-load-state\u001b[0m\n",
            "[ 78%] Built target llama-save-load-state\n",
            "[ 79%] \u001b[32mBuilding CXX object examples/run/CMakeFiles/llama-run.dir/run.cpp.o\u001b[0m\n",
            "[ 79%] \u001b[32mBuilding CXX object examples/run/CMakeFiles/llama-run.dir/linenoise.cpp/linenoise.cpp.o\u001b[0m\n",
            "[ 80%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-run\u001b[0m\n",
            "[ 80%] Built target llama-run\n",
            "[ 81%] \u001b[32mBuilding CXX object examples/simple/CMakeFiles/llama-simple.dir/simple.cpp.o\u001b[0m\n",
            "[ 81%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-simple\u001b[0m\n",
            "[ 81%] Built target llama-simple\n",
            "[ 82%] \u001b[32mBuilding CXX object examples/simple-chat/CMakeFiles/llama-simple-chat.dir/simple-chat.cpp.o\u001b[0m\n",
            "[ 82%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-simple-chat\u001b[0m\n",
            "[ 82%] Built target llama-simple-chat\n",
            "[ 83%] \u001b[32mBuilding CXX object examples/speculative/CMakeFiles/llama-speculative.dir/speculative.cpp.o\u001b[0m\n",
            "[ 83%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-speculative\u001b[0m\n",
            "[ 83%] Built target llama-speculative\n",
            "[ 84%] \u001b[32mBuilding CXX object examples/speculative-simple/CMakeFiles/llama-speculative-simple.dir/speculative-simple.cpp.o\u001b[0m\n",
            "[ 84%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-speculative-simple\u001b[0m\n",
            "[ 84%] Built target llama-speculative-simple\n",
            "[ 84%] \u001b[32mBuilding CXX object examples/tokenize/CMakeFiles/llama-tokenize.dir/tokenize.cpp.o\u001b[0m\n",
            "[ 85%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-tokenize\u001b[0m\n",
            "[ 85%] Built target llama-tokenize\n",
            "[ 85%] \u001b[32mBuilding CXX object examples/tts/CMakeFiles/llama-tts.dir/tts.cpp.o\u001b[0m\n",
            "[ 86%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-tts\u001b[0m\n",
            "[ 86%] Built target llama-tts\n",
            "[ 87%] \u001b[32mBuilding CXX object examples/gen-docs/CMakeFiles/llama-gen-docs.dir/gen-docs.cpp.o\u001b[0m\n",
            "[ 87%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-gen-docs\u001b[0m\n",
            "[ 87%] Built target llama-gen-docs\n",
            "[ 87%] \u001b[32mBuilding CXX object examples/convert-llama2c-to-ggml/CMakeFiles/llama-convert-llama2c-to-ggml.dir/convert-llama2c-to-ggml.cpp.o\u001b[0m\n",
            "[ 88%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-convert-llama2c-to-ggml\u001b[0m\n",
            "[ 88%] Built target llama-convert-llama2c-to-ggml\n",
            "[ 88%] \u001b[32mBuilding CXX object examples/cvector-generator/CMakeFiles/llama-cvector-generator.dir/cvector-generator.cpp.o\u001b[0m\n",
            "[ 89%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-cvector-generator\u001b[0m\n",
            "[ 89%] Built target llama-cvector-generator\n",
            "[ 89%] \u001b[32mBuilding CXX object examples/export-lora/CMakeFiles/llama-export-lora.dir/export-lora.cpp.o\u001b[0m\n",
            "[ 90%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-export-lora\u001b[0m\n",
            "[ 90%] Built target llama-export-lora\n",
            "[ 90%] \u001b[32mBuilding CXX object examples/quantize-stats/CMakeFiles/llama-quantize-stats.dir/quantize-stats.cpp.o\u001b[0m\n",
            "[ 90%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-quantize-stats\u001b[0m\n",
            "[ 90%] Built target llama-quantize-stats\n",
            "[ 90%] \u001b[32mBuilding CXX object examples/llava/CMakeFiles/llava.dir/llava.cpp.o\u001b[0m\n",
            "[ 91%] \u001b[32mBuilding CXX object examples/llava/CMakeFiles/llava.dir/clip.cpp.o\u001b[0m\n",
            "[ 91%] Built target llava\n",
            "[ 92%] \u001b[32m\u001b[1mLinking CXX static library libllava_static.a\u001b[0m\n",
            "[ 92%] Built target llava_static\n",
            "[ 92%] \u001b[32m\u001b[1mLinking CXX shared library ../../bin/libllava_shared.so\u001b[0m\n",
            "[ 92%] Built target llava_shared\n",
            "[ 92%] \u001b[32mBuilding CXX object examples/llava/CMakeFiles/mtmd.dir/mtmd.cpp.o\u001b[0m\n",
            "[ 92%] \u001b[32mBuilding CXX object examples/llava/CMakeFiles/mtmd.dir/clip.cpp.o\u001b[0m\n",
            "[ 92%] Built target mtmd\n",
            "[ 92%] \u001b[32m\u001b[1mLinking CXX static library libmtmd_static.a\u001b[0m\n",
            "[ 92%] Built target mtmd_static\n",
            "[ 93%] \u001b[32m\u001b[1mLinking CXX shared library ../../bin/libmtmd_shared.so\u001b[0m\n",
            "[ 93%] Built target mtmd_shared\n",
            "[ 93%] \u001b[32mBuilding CXX object examples/llava/CMakeFiles/llama-llava-cli.dir/llava-cli.cpp.o\u001b[0m\n",
            "[ 94%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-llava-cli\u001b[0m\n",
            "[ 94%] Built target llama-llava-cli\n",
            "[ 95%] \u001b[32mBuilding CXX object examples/llava/CMakeFiles/llama-minicpmv-cli.dir/minicpmv-cli.cpp.o\u001b[0m\n",
            "[ 95%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-minicpmv-cli\u001b[0m\n",
            "[ 95%] Built target llama-minicpmv-cli\n",
            "[ 96%] \u001b[32mBuilding CXX object examples/llava/CMakeFiles/llama-qwen2vl-cli.dir/qwen2vl-cli.cpp.o\u001b[0m\n",
            "[ 96%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-qwen2vl-cli\u001b[0m\n",
            "[ 96%] Built target llama-qwen2vl-cli\n",
            "[ 97%] \u001b[32mBuilding CXX object examples/llava/CMakeFiles/llama-gemma3-cli.dir/gemma3-cli.cpp.o\u001b[0m\n",
            "[ 97%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-gemma3-cli\u001b[0m\n",
            "[ 97%] Built target llama-gemma3-cli\n",
            "[ 97%] \u001b[32mBuilding CXX object examples/llava/CMakeFiles/llama-llava-clip-quantize-cli.dir/clip-quantize-cli.cpp.o\u001b[0m\n",
            "[ 98%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-llava-clip-quantize-cli\u001b[0m\n",
            "[ 98%] Built target llama-llava-clip-quantize-cli\n",
            "[ 98%] \u001b[32mBuilding CXX object pocs/vdot/CMakeFiles/llama-vdot.dir/vdot.cpp.o\u001b[0m\n",
            "[ 99%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-vdot\u001b[0m\n",
            "[ 99%] Built target llama-vdot\n",
            "[ 99%] \u001b[32mBuilding CXX object pocs/vdot/CMakeFiles/llama-q8dot.dir/q8dot.cpp.o\u001b[0m\n",
            "[100%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-q8dot\u001b[0m\n",
            "[100%] Built target llama-q8dot\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "os.chdir(\"llama.cpp/\")"
      ],
      "metadata": {
        "id": "jXJOrfDiOBMk"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!build/bin/llama-quantize deepseek_south_ostrobothnia.gguf deepseek_south_ostrobothnia.Q5_K_M.gguf Q5_K_M\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wZyLFQE_NujD",
        "outputId": "ee5818b4-9b9b-442c-e811-582097de39e4"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "main: build = 5124 (bc091a4d)\n",
            "main: built with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for x86_64-linux-gnu\n",
            "main: quantizing 'deepseek_south_ostrobothnia.gguf' to 'deepseek_medical.Q5_K_M.gguf' as Q5_K_M\n",
            "llama_model_loader: loaded meta data with 34 key-value pairs and 292 tensors from deepseek_south_ostrobothnia.gguf (version GGUF V3 (latest))\n",
            "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
            "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
            "llama_model_loader: - kv   1:                               general.type str              = model\n",
            "llama_model_loader: - kv   2:                               general.name str              = DeepSeek R1 South Ostrobothnia\n",
            "llama_model_loader: - kv   3:                         general.size_label str              = 8.0B\n",
            "llama_model_loader: - kv   4:                   general.base_model.count u32              = 1\n",
            "llama_model_loader: - kv   5:                  general.base_model.0.name str              = Deepseek R1 Distill Llama 8b Unsloth ...\n",
            "llama_model_loader: - kv   6:          general.base_model.0.organization str              = Unsloth\n",
            "llama_model_loader: - kv   7:              general.base_model.0.repo_url str              = https://huggingface.co/unsloth/deepse...\n",
            "llama_model_loader: - kv   8:                          llama.block_count u32              = 32\n",
            "llama_model_loader: - kv   9:                       llama.context_length u32              = 131072\n",
            "llama_model_loader: - kv  10:                     llama.embedding_length u32              = 4096\n",
            "llama_model_loader: - kv  11:                  llama.feed_forward_length u32              = 14336\n",
            "llama_model_loader: - kv  12:                 llama.attention.head_count u32              = 32\n",
            "llama_model_loader: - kv  13:              llama.attention.head_count_kv u32              = 8\n",
            "llama_model_loader: - kv  14:                       llama.rope.freq_base f32              = 500000.000000\n",
            "llama_model_loader: - kv  15:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
            "llama_model_loader: - kv  16:                 llama.attention.key_length u32              = 128\n",
            "llama_model_loader: - kv  17:               llama.attention.value_length u32              = 128\n",
            "llama_model_loader: - kv  18:                          general.file_type u32              = 1\n",
            "llama_model_loader: - kv  19:                           llama.vocab_size u32              = 128256\n",
            "llama_model_loader: - kv  20:                 llama.rope.dimension_count u32              = 128\n",
            "llama_model_loader: - kv  21:                       tokenizer.ggml.model str              = gpt2\n",
            "llama_model_loader: - kv  22:                         tokenizer.ggml.pre str              = llama-bpe\n",
            "llama_model_loader: - kv  23:                      tokenizer.ggml.tokens arr[str,128256]  = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"'\", ...\n",
            "llama_model_loader: - kv  24:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\n",
            "llama_model_loader: - kv  25:                      tokenizer.ggml.merges arr[str,280147]  = [\"Ġ Ġ\", \"Ġ ĠĠĠ\", \"ĠĠ ĠĠ\", \"...\n",
            "llama_model_loader: - kv  26:                tokenizer.ggml.bos_token_id u32              = 128000\n",
            "llama_model_loader: - kv  27:                tokenizer.ggml.eos_token_id u32              = 128001\n",
            "llama_model_loader: - kv  28:            tokenizer.ggml.padding_token_id u32              = 128004\n",
            "llama_model_loader: - kv  29:               tokenizer.ggml.add_bos_token bool             = true\n",
            "llama_model_loader: - kv  30:               tokenizer.ggml.add_eos_token bool             = false\n",
            "llama_model_loader: - kv  31:                    tokenizer.chat_template str              = {% if not add_generation_prompt is de...\n",
            "llama_model_loader: - kv  32:            tokenizer.ggml.add_space_prefix bool             = false\n",
            "llama_model_loader: - kv  33:               general.quantization_version u32              = 2\n",
            "llama_model_loader: - type  f32:   66 tensors\n",
            "llama_model_loader: - type  f16:  226 tensors\n",
            "[   1/ 292]                        output.weight - [ 4096, 128256,     1,     1], type =    f16, converting to q6_K .. size =  1002.00 MiB ->   410.98 MiB\n",
            "[   2/ 292]                   output_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[   3/ 292]                    rope_freqs.weight - [   64,     1,     1,     1], type =    f32, size =    0.000 MB\n",
            "[   4/ 292]                    token_embd.weight - [ 4096, 128256,     1,     1], type =    f16, converting to q5_K .. size =  1002.00 MiB ->   344.44 MiB\n",
            "[   5/ 292]                  blk.0.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB\n",
            "[   6/ 292]               blk.0.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[   7/ 292]             blk.0.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB\n",
            "[   8/ 292]                  blk.0.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB\n",
            "[   9/ 292]                  blk.0.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q6_K .. size =     8.00 MiB ->     3.28 MiB\n",
            "[  10/ 292]                blk.0.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q6_K .. size =   112.00 MiB ->    45.94 MiB\n",
            "[  11/ 292]                blk.0.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q5_K .. size =   112.00 MiB ->    38.50 MiB\n",
            "[  12/ 292]                blk.0.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[  13/ 292]                  blk.0.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q5_K .. size =   112.00 MiB ->    38.50 MiB\n",
            "[  14/ 292]                  blk.1.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB\n",
            "[  15/ 292]               blk.1.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[  16/ 292]             blk.1.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB\n",
            "[  17/ 292]                  blk.1.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB\n",
            "[  18/ 292]                  blk.1.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q6_K .. size =     8.00 MiB ->     3.28 MiB\n",
            "[  19/ 292]                blk.1.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q6_K .. size =   112.00 MiB ->    45.94 MiB\n",
            "[  20/ 292]                blk.1.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q5_K .. size =   112.00 MiB ->    38.50 MiB\n",
            "[  21/ 292]                blk.1.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[  22/ 292]                  blk.1.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q5_K .. size =   112.00 MiB ->    38.50 MiB\n",
            "[  23/ 292]                  blk.2.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB\n",
            "[  24/ 292]               blk.2.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[  25/ 292]             blk.2.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB\n",
            "[  26/ 292]                  blk.2.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB\n",
            "[  27/ 292]                  blk.2.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q6_K .. size =     8.00 MiB ->     3.28 MiB\n",
            "[  28/ 292]                blk.2.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q6_K .. size =   112.00 MiB ->    45.94 MiB\n",
            "[  29/ 292]                blk.2.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q5_K .. size =   112.00 MiB ->    38.50 MiB\n",
            "[  30/ 292]                blk.2.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[  31/ 292]                  blk.2.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q5_K .. size =   112.00 MiB ->    38.50 MiB\n",
            "[  32/ 292]                  blk.3.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB\n",
            "[  33/ 292]               blk.3.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[  34/ 292]             blk.3.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB\n",
            "[  35/ 292]                  blk.3.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB\n",
            "[  36/ 292]                  blk.3.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q6_K .. size =     8.00 MiB ->     3.28 MiB\n",
            "[  37/ 292]                blk.3.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q6_K .. size =   112.00 MiB ->    45.94 MiB\n",
            "[  38/ 292]                blk.3.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q5_K .. size =   112.00 MiB ->    38.50 MiB\n",
            "[  39/ 292]                blk.3.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[  40/ 292]                  blk.3.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q5_K .. size =   112.00 MiB ->    38.50 MiB\n",
            "[  41/ 292]                  blk.4.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB\n",
            "[  42/ 292]               blk.4.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[  43/ 292]             blk.4.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB\n",
            "[  44/ 292]                  blk.4.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB\n",
            "[  45/ 292]                  blk.4.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB\n",
            "[  46/ 292]                blk.4.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q5_K .. size =   112.00 MiB ->    38.50 MiB\n",
            "[  47/ 292]                blk.4.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q5_K .. size =   112.00 MiB ->    38.50 MiB\n",
            "[  48/ 292]                blk.4.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[  49/ 292]                  blk.4.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q5_K .. size =   112.00 MiB ->    38.50 MiB\n",
            "[  50/ 292]                  blk.5.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB\n",
            "[  51/ 292]               blk.5.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[  52/ 292]             blk.5.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB\n",
            "[  53/ 292]                  blk.5.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB\n",
            "[  54/ 292]                  blk.5.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB\n",
            "[  55/ 292]                blk.5.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q5_K .. size =   112.00 MiB ->    38.50 MiB\n",
            "[  56/ 292]                blk.5.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q5_K .. size =   112.00 MiB ->    38.50 MiB\n",
            "[  57/ 292]                blk.5.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[  58/ 292]                  blk.5.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q5_K .. size =   112.00 MiB ->    38.50 MiB\n",
            "[  59/ 292]                  blk.6.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB\n",
            "[  60/ 292]               blk.6.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[  61/ 292]             blk.6.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB\n",
            "[  62/ 292]                  blk.6.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB\n",
            "[  63/ 292]                  blk.6.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q6_K .. size =     8.00 MiB ->     3.28 MiB\n",
            "[  64/ 292]                blk.6.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q6_K .. size =   112.00 MiB ->    45.94 MiB\n",
            "[  65/ 292]                blk.6.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q5_K .. size =   112.00 MiB ->    38.50 MiB\n",
            "[  66/ 292]                blk.6.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[  67/ 292]                  blk.6.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q5_K .. size =   112.00 MiB ->    38.50 MiB\n",
            "[  68/ 292]                  blk.7.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB\n",
            "[  69/ 292]               blk.7.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[  70/ 292]             blk.7.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB\n",
            "[  71/ 292]                  blk.7.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB\n",
            "[  72/ 292]                  blk.7.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB\n",
            "[  73/ 292]                blk.7.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q5_K .. size =   112.00 MiB ->    38.50 MiB\n",
            "[  74/ 292]                blk.7.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q5_K .. size =   112.00 MiB ->    38.50 MiB\n",
            "[  75/ 292]                blk.7.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[  76/ 292]                  blk.7.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q5_K .. size =   112.00 MiB ->    38.50 MiB\n",
            "[  77/ 292]                  blk.8.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB\n",
            "[  78/ 292]               blk.8.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[  79/ 292]             blk.8.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB\n",
            "[  80/ 292]                  blk.8.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB\n",
            "[  81/ 292]                  blk.8.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB\n",
            "[  82/ 292]                blk.8.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q5_K .. size =   112.00 MiB ->    38.50 MiB\n",
            "[  83/ 292]                blk.8.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q5_K .. size =   112.00 MiB ->    38.50 MiB\n",
            "[  84/ 292]                blk.8.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[  85/ 292]                  blk.8.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q5_K .. size =   112.00 MiB ->    38.50 MiB\n",
            "[  86/ 292]                  blk.9.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB\n",
            "[  87/ 292]               blk.9.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[  88/ 292]             blk.9.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB\n",
            "[  89/ 292]                  blk.9.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB\n",
            "[  90/ 292]                  blk.9.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q6_K .. size =     8.00 MiB ->     3.28 MiB\n",
            "[  91/ 292]                blk.9.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q6_K .. size =   112.00 MiB ->    45.94 MiB\n",
            "[  92/ 292]                blk.9.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q5_K .. size =   112.00 MiB ->    38.50 MiB\n",
            "[  93/ 292]                blk.9.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[  94/ 292]                  blk.9.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q5_K .. size =   112.00 MiB ->    38.50 MiB\n",
            "[  95/ 292]                 blk.10.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB\n",
            "[  96/ 292]              blk.10.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[  97/ 292]            blk.10.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB\n",
            "[  98/ 292]                 blk.10.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB\n",
            "[  99/ 292]                 blk.10.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB\n",
            "[ 100/ 292]               blk.10.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q5_K .. size =   112.00 MiB ->    38.50 MiB\n",
            "[ 101/ 292]               blk.10.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q5_K .. size =   112.00 MiB ->    38.50 MiB\n",
            "[ 102/ 292]               blk.10.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 103/ 292]                 blk.10.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q5_K .. size =   112.00 MiB ->    38.50 MiB\n",
            "[ 104/ 292]                 blk.11.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB\n",
            "[ 105/ 292]              blk.11.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 106/ 292]            blk.11.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB\n",
            "[ 107/ 292]                 blk.11.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB\n",
            "[ 108/ 292]                 blk.11.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB\n",
            "[ 109/ 292]               blk.11.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q5_K .. size =   112.00 MiB ->    38.50 MiB\n",
            "[ 110/ 292]               blk.11.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q5_K .. size =   112.00 MiB ->    38.50 MiB\n",
            "[ 111/ 292]               blk.11.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 112/ 292]                 blk.11.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q5_K .. size =   112.00 MiB ->    38.50 MiB\n",
            "[ 113/ 292]                 blk.12.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB\n",
            "[ 114/ 292]              blk.12.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 115/ 292]            blk.12.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB\n",
            "[ 116/ 292]                 blk.12.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB\n",
            "[ 117/ 292]                 blk.12.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q6_K .. size =     8.00 MiB ->     3.28 MiB\n",
            "[ 118/ 292]               blk.12.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q6_K .. size =   112.00 MiB ->    45.94 MiB\n",
            "[ 119/ 292]               blk.12.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q5_K .. size =   112.00 MiB ->    38.50 MiB\n",
            "[ 120/ 292]               blk.12.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 121/ 292]                 blk.12.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q5_K .. size =   112.00 MiB ->    38.50 MiB\n",
            "[ 122/ 292]                 blk.13.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB\n",
            "[ 123/ 292]              blk.13.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 124/ 292]            blk.13.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB\n",
            "[ 125/ 292]                 blk.13.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB\n",
            "[ 126/ 292]                 blk.13.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB\n",
            "[ 127/ 292]               blk.13.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q5_K .. size =   112.00 MiB ->    38.50 MiB\n",
            "[ 128/ 292]               blk.13.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q5_K .. size =   112.00 MiB ->    38.50 MiB\n",
            "[ 129/ 292]               blk.13.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 130/ 292]                 blk.13.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q5_K .. size =   112.00 MiB ->    38.50 MiB\n",
            "[ 131/ 292]                 blk.14.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB\n",
            "[ 132/ 292]              blk.14.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 133/ 292]            blk.14.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB\n",
            "[ 134/ 292]                 blk.14.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB\n",
            "[ 135/ 292]                 blk.14.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB\n",
            "[ 136/ 292]               blk.14.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q5_K .. size =   112.00 MiB ->    38.50 MiB\n",
            "[ 137/ 292]               blk.14.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q5_K .. size =   112.00 MiB ->    38.50 MiB\n",
            "[ 138/ 292]               blk.14.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 139/ 292]                 blk.14.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q5_K .. size =   112.00 MiB ->    38.50 MiB\n",
            "[ 140/ 292]                 blk.15.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB\n",
            "[ 141/ 292]              blk.15.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 142/ 292]            blk.15.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB\n",
            "[ 143/ 292]                 blk.15.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB\n",
            "[ 144/ 292]                 blk.15.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q6_K .. size =     8.00 MiB ->     3.28 MiB\n",
            "[ 145/ 292]               blk.15.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q6_K .. size =   112.00 MiB ->    45.94 MiB\n",
            "[ 146/ 292]               blk.15.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q5_K .. size =   112.00 MiB ->    38.50 MiB\n",
            "[ 147/ 292]               blk.15.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 148/ 292]                 blk.15.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q5_K .. size =   112.00 MiB ->    38.50 MiB\n",
            "[ 149/ 292]                 blk.16.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB\n",
            "[ 150/ 292]              blk.16.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 151/ 292]            blk.16.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB\n",
            "[ 152/ 292]                 blk.16.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB\n",
            "[ 153/ 292]                 blk.16.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB\n",
            "[ 154/ 292]               blk.16.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q5_K .. size =   112.00 MiB ->    38.50 MiB\n",
            "[ 155/ 292]               blk.16.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q5_K .. size =   112.00 MiB ->    38.50 MiB\n",
            "[ 156/ 292]               blk.16.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 157/ 292]                 blk.16.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q5_K .. size =   112.00 MiB ->    38.50 MiB\n",
            "[ 158/ 292]                 blk.17.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB\n",
            "[ 159/ 292]              blk.17.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 160/ 292]            blk.17.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB\n",
            "[ 161/ 292]                 blk.17.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB\n",
            "[ 162/ 292]                 blk.17.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB\n",
            "[ 163/ 292]               blk.17.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q5_K .. size =   112.00 MiB ->    38.50 MiB\n",
            "[ 164/ 292]               blk.17.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q5_K .. size =   112.00 MiB ->    38.50 MiB\n",
            "[ 165/ 292]               blk.17.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 166/ 292]                 blk.17.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q5_K .. size =   112.00 MiB ->    38.50 MiB\n",
            "[ 167/ 292]                 blk.18.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB\n",
            "[ 168/ 292]              blk.18.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 169/ 292]            blk.18.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB\n",
            "[ 170/ 292]                 blk.18.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB\n",
            "[ 171/ 292]                 blk.18.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q6_K .. size =     8.00 MiB ->     3.28 MiB\n",
            "[ 172/ 292]               blk.18.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q6_K .. size =   112.00 MiB ->    45.94 MiB\n",
            "[ 173/ 292]               blk.18.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q5_K .. size =   112.00 MiB ->    38.50 MiB\n",
            "[ 174/ 292]               blk.18.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 175/ 292]                 blk.18.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q5_K .. size =   112.00 MiB ->    38.50 MiB\n",
            "[ 176/ 292]                 blk.19.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB\n",
            "[ 177/ 292]              blk.19.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 178/ 292]            blk.19.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB\n",
            "[ 179/ 292]                 blk.19.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB\n",
            "[ 180/ 292]                 blk.19.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB\n",
            "[ 181/ 292]               blk.19.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q5_K .. size =   112.00 MiB ->    38.50 MiB\n",
            "[ 182/ 292]               blk.19.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q5_K .. size =   112.00 MiB ->    38.50 MiB\n",
            "[ 183/ 292]               blk.19.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 184/ 292]                 blk.19.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q5_K .. size =   112.00 MiB ->    38.50 MiB\n",
            "[ 185/ 292]                 blk.20.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB\n",
            "[ 186/ 292]              blk.20.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 187/ 292]            blk.20.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB\n",
            "[ 188/ 292]                 blk.20.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB\n",
            "[ 189/ 292]                 blk.20.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB\n",
            "[ 190/ 292]               blk.20.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q5_K .. size =   112.00 MiB ->    38.50 MiB\n",
            "[ 191/ 292]               blk.20.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q5_K .. size =   112.00 MiB ->    38.50 MiB\n",
            "[ 192/ 292]               blk.20.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 193/ 292]                 blk.20.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q5_K .. size =   112.00 MiB ->    38.50 MiB\n",
            "[ 194/ 292]                 blk.21.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB\n",
            "[ 195/ 292]              blk.21.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 196/ 292]            blk.21.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB\n",
            "[ 197/ 292]                 blk.21.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB\n",
            "[ 198/ 292]                 blk.21.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q6_K .. size =     8.00 MiB ->     3.28 MiB\n",
            "[ 199/ 292]               blk.21.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q6_K .. size =   112.00 MiB ->    45.94 MiB\n",
            "[ 200/ 292]               blk.21.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q5_K .. size =   112.00 MiB ->    38.50 MiB\n",
            "[ 201/ 292]               blk.21.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 202/ 292]                 blk.21.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q5_K .. size =   112.00 MiB ->    38.50 MiB\n",
            "[ 203/ 292]                 blk.22.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB\n",
            "[ 204/ 292]              blk.22.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 205/ 292]            blk.22.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB\n",
            "[ 206/ 292]                 blk.22.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB\n",
            "[ 207/ 292]                 blk.22.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB\n",
            "[ 208/ 292]               blk.22.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q5_K .. size =   112.00 MiB ->    38.50 MiB\n",
            "[ 209/ 292]               blk.22.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q5_K .. size =   112.00 MiB ->    38.50 MiB\n",
            "[ 210/ 292]               blk.22.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 211/ 292]                 blk.22.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q5_K .. size =   112.00 MiB ->    38.50 MiB\n",
            "[ 212/ 292]                 blk.23.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB\n",
            "[ 213/ 292]              blk.23.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 214/ 292]            blk.23.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB\n",
            "[ 215/ 292]                 blk.23.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB\n",
            "[ 216/ 292]                 blk.23.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB\n",
            "[ 217/ 292]               blk.23.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q5_K .. size =   112.00 MiB ->    38.50 MiB\n",
            "[ 218/ 292]               blk.23.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q5_K .. size =   112.00 MiB ->    38.50 MiB\n",
            "[ 219/ 292]               blk.23.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 220/ 292]                 blk.23.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q5_K .. size =   112.00 MiB ->    38.50 MiB\n",
            "[ 221/ 292]                 blk.24.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB\n",
            "[ 222/ 292]              blk.24.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 223/ 292]            blk.24.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB\n",
            "[ 224/ 292]                 blk.24.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB\n",
            "[ 225/ 292]                 blk.24.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q6_K .. size =     8.00 MiB ->     3.28 MiB\n",
            "[ 226/ 292]               blk.24.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q6_K .. size =   112.00 MiB ->    45.94 MiB\n",
            "[ 227/ 292]               blk.24.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q5_K .. size =   112.00 MiB ->    38.50 MiB\n",
            "[ 228/ 292]               blk.24.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 229/ 292]                 blk.24.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q5_K .. size =   112.00 MiB ->    38.50 MiB\n",
            "[ 230/ 292]                 blk.25.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB\n",
            "[ 231/ 292]              blk.25.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 232/ 292]            blk.25.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB\n",
            "[ 233/ 292]                 blk.25.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB\n",
            "[ 234/ 292]                 blk.25.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB\n",
            "[ 235/ 292]               blk.25.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q5_K .. size =   112.00 MiB ->    38.50 MiB\n",
            "[ 236/ 292]               blk.25.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q5_K .. size =   112.00 MiB ->    38.50 MiB\n",
            "[ 237/ 292]               blk.25.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 238/ 292]                 blk.25.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q5_K .. size =   112.00 MiB ->    38.50 MiB\n",
            "[ 239/ 292]                 blk.26.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB\n",
            "[ 240/ 292]              blk.26.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 241/ 292]            blk.26.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB\n",
            "[ 242/ 292]                 blk.26.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB\n",
            "[ 243/ 292]                 blk.26.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB\n",
            "[ 244/ 292]               blk.26.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q5_K .. size =   112.00 MiB ->    38.50 MiB\n",
            "[ 245/ 292]               blk.26.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q5_K .. size =   112.00 MiB ->    38.50 MiB\n",
            "[ 246/ 292]               blk.26.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 247/ 292]                 blk.26.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q5_K .. size =   112.00 MiB ->    38.50 MiB\n",
            "[ 248/ 292]                 blk.27.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB\n",
            "[ 249/ 292]              blk.27.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 250/ 292]            blk.27.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB\n",
            "[ 251/ 292]                 blk.27.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB\n",
            "[ 252/ 292]                 blk.27.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q6_K .. size =     8.00 MiB ->     3.28 MiB\n",
            "[ 253/ 292]               blk.27.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q6_K .. size =   112.00 MiB ->    45.94 MiB\n",
            "[ 254/ 292]               blk.27.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q5_K .. size =   112.00 MiB ->    38.50 MiB\n",
            "[ 255/ 292]               blk.27.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 256/ 292]                 blk.27.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q5_K .. size =   112.00 MiB ->    38.50 MiB\n",
            "[ 257/ 292]                 blk.28.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB\n",
            "[ 258/ 292]              blk.28.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 259/ 292]            blk.28.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB\n",
            "[ 260/ 292]                 blk.28.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB\n",
            "[ 261/ 292]                 blk.28.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q6_K .. size =     8.00 MiB ->     3.28 MiB\n",
            "[ 262/ 292]               blk.28.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q6_K .. size =   112.00 MiB ->    45.94 MiB\n",
            "[ 263/ 292]               blk.28.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q5_K .. size =   112.00 MiB ->    38.50 MiB\n",
            "[ 264/ 292]               blk.28.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 265/ 292]                 blk.28.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q5_K .. size =   112.00 MiB ->    38.50 MiB\n",
            "[ 266/ 292]                 blk.29.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB\n",
            "[ 267/ 292]              blk.29.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 268/ 292]            blk.29.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB\n",
            "[ 269/ 292]                 blk.29.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB\n",
            "[ 270/ 292]                 blk.29.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q6_K .. size =     8.00 MiB ->     3.28 MiB\n",
            "[ 271/ 292]               blk.29.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q6_K .. size =   112.00 MiB ->    45.94 MiB\n",
            "[ 272/ 292]               blk.29.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q5_K .. size =   112.00 MiB ->    38.50 MiB\n",
            "[ 273/ 292]               blk.29.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 274/ 292]                 blk.29.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q5_K .. size =   112.00 MiB ->    38.50 MiB\n",
            "[ 275/ 292]                 blk.30.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB\n",
            "[ 276/ 292]              blk.30.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 277/ 292]            blk.30.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB\n",
            "[ 278/ 292]                 blk.30.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB\n",
            "[ 279/ 292]                 blk.30.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q6_K .. size =     8.00 MiB ->     3.28 MiB\n",
            "[ 280/ 292]               blk.30.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q6_K .. size =   112.00 MiB ->    45.94 MiB\n",
            "[ 281/ 292]               blk.30.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q5_K .. size =   112.00 MiB ->    38.50 MiB\n",
            "[ 282/ 292]               blk.30.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 283/ 292]                 blk.30.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q5_K .. size =   112.00 MiB ->    38.50 MiB\n",
            "[ 284/ 292]                 blk.31.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB\n",
            "[ 285/ 292]              blk.31.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 286/ 292]            blk.31.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB\n",
            "[ 287/ 292]                 blk.31.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB\n",
            "[ 288/ 292]                 blk.31.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q6_K .. size =     8.00 MiB ->     3.28 MiB\n",
            "[ 289/ 292]               blk.31.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q6_K .. size =   112.00 MiB ->    45.94 MiB\n",
            "[ 290/ 292]               blk.31.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q5_K .. size =   112.00 MiB ->    38.50 MiB\n",
            "[ 291/ 292]               blk.31.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 292/ 292]                 blk.31.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q5_K .. size =   112.00 MiB ->    38.50 MiB\n",
            "llama_model_quantize_impl: model size  = 15317.02 MB\n",
            "llama_model_quantize_impl: quant size  =  5459.93 MB\n",
            "\n",
            "main: quantize time = 780705.01 ms\n",
            "main:    total time = 780705.02 ms\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "!ls\n",
        "!cp deepseek_south_ostrobothnia.Q5_K_M.gguf \"/content/drive/My Drive/models\"\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ypoRzZ8iQsU3",
        "outputId": "6aa2c9d9-91e1-4283-8b5b-b3ade75ec7c7"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "AUTHORS\t\t\t\t  gguf-py\n",
            "build\t\t\t\t  grammars\n",
            "build-xcframework.sh\t\t  include\n",
            "ci\t\t\t\t  LICENSE\n",
            "cmake\t\t\t\t  licenses\n",
            "CMakeLists.txt\t\t\t  Makefile\n",
            "CMakePresets.json\t\t  media\n",
            "CODEOWNERS\t\t\t  models\n",
            "common\t\t\t\t  mypy.ini\n",
            "CONTRIBUTING.md\t\t\t  pocs\n",
            "convert_hf_to_gguf.py\t\t  poetry.lock\n",
            "convert_hf_to_gguf_update.py\t  prompts\n",
            "convert_llama_ggml_to_gguf.py\t  pyproject.toml\n",
            "convert_lora_to_gguf.py\t\t  pyrightconfig.json\n",
            "deepseek_medical.Q5_K_M.gguf\t  README.md\n",
            "deepseek_south_ostrobothnia.gguf  requirements\n",
            "docs\t\t\t\t  requirements.txt\n",
            "examples\t\t\t  scripts\n",
            "flake.lock\t\t\t  SECURITY.md\n",
            "flake.nix\t\t\t  src\n",
            "ggml\t\t\t\t  tests\n",
            "cp: cannot stat 'deepseek_south_ostrobothnia.Q5_K_M.gguf': No such file or directory\n"
          ]
        }
      ]
=======
            "Hevooskauppias ajoo uureella hevoosella, meirän talo ohi, kattoon sitä aita ovelta<｜end▁of▁sentence｜>\n"
          ]
        }
      ],
      "execution_count": 27
>>>>>>> 1e7dd18e3a6bd07f6a646df9a95feae4621933ae
    }
  ]
}